{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model extraction -  geo data using open Ai"
      ],
      "metadata": {
        "id": "sHCbot2Ewmud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: PDF Loader & Page Windowing"
      ],
      "metadata": {
        "id": "tdJmRsUvwv2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracted all 257 pages of text from your thesis using pypdf.\n",
        "\n",
        "Applied smart windowing rules: started at the first “Introduction” page (never before page 17–19).\n",
        "\n",
        "Handled multiple references: when a “References/Bibliography” page is hit, the code pauses, looks ahead 4–6 pages—\n",
        "\n",
        "If another section header (e.g., “Chapter IV”) is found, it skips the references and resumes extraction.\n",
        "\n",
        "If not, it stops for good at the final references.\n",
        "\n",
        "Created a clean subset (selected_pages_text) with only the relevant content for downstream extraction, excluding chapter-level reference lists and back-matter noise."
      ],
      "metadata": {
        "id": "ArPzsFlYxd73"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYddcrrFotEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c720d52c-b009-4100-88c2-76b475257408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 257 pages from: /content/PhDThesis__Masurel2015.pdf\n",
            "\n",
            "=== SMART WINDOW SUMMARY ===\n",
            "Total pages: 257\n",
            "Selected pages: 234\n",
            "From page 19 to 252\n",
            "\n",
            "--- Start page preview ---\n",
            "    1    Chapter I. Introduction     1. Preamble and knowledge gaps    Paleoproterozoic (i.e. Biri mian) volcano-plutonic belts and sedimentary basins  of West Africa not only  provide a complete record of crustal growth but also host a  number of world-class gold deposits (Abouc hami et al., 1990; Boher et al., 1992). To  date, a large number of studies have focused on the Baoulé-Mossi domain, which covers  portions of Burkina Faso, Côte d’Ivoire, Ghana, Guinea and Mali (Fig. 1). Gold  deposits\n",
            "\n",
            "--- End page preview ---\n",
            "    234    3. Future work    Despite the work undertaken by the candi date, a number of questions remain  unanswered, some of which are directly relevant to exploration targeting:     3.1. Source of fluids and metals    Recent research suggests that the divers ity in mineralisation styles and ore  paragenesis expressed in the KKI result from a dynamic hydrothermal system that  sourced fluids and metals from both metamo rphic and magmatic reservoirs (Lawrence  et al., 2013a, b; Treloar et al., 20\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip -q install pypdf\n",
        "\n",
        "import re\n",
        "from pathlib import Path\n",
        "from pypdf import PdfReader\n",
        "\n",
        "PDF_PATH = \"/content/PhDThesis__Masurel2015.pdf\"\n",
        "\n",
        "assert PDF_PATH is not None and Path(PDF_PATH).exists(), \"PDF_PATH not set or file not found.\"\n",
        "\n",
        "# === Config ===\n",
        "MIN_SKIP_PAGES = 17\n",
        "MAX_SKIP_PAGES_FALLBACK = 19\n",
        "TAIL_EXCLUDE_PAGES = 5\n",
        "LOOKAHEAD_PAGES = 6  # how far to look ahead after \"References\"\n",
        "\n",
        "INTRO_PATTERNS = [\n",
        "    r'^\\s*(chapter\\s+\\w+\\.?\\s+)?introduction\\b',\n",
        "    r'^\\s*introduction\\b',\n",
        "]\n",
        "STOP_HEADINGS = [\n",
        "    r'^\\s*references\\b',\n",
        "    r'^\\s*bibliograph(y|ies)\\b',\n",
        "    r'^\\s*acknowledg(e)?ments\\b',\n",
        "]\n",
        "SECTION_HEADINGS = [\n",
        "    r'^\\s*chapter\\s+[ivxlcdm]+\\b',\n",
        "    r'^\\s*chapter\\s+\\d+\\b',\n",
        "    r'^\\s*\\d+\\.\\s+[A-Z]',\n",
        "]\n",
        "\n",
        "# === Helpers ===\n",
        "def read_pdf_pages_text(pdf_path: str):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    return [page.extract_text() or \"\" for page in reader.pages]\n",
        "\n",
        "def match_any(text, patterns):\n",
        "    for pat in patterns:\n",
        "        if re.search(pat, text, flags=re.IGNORECASE | re.MULTILINE):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def first_intro_page(pages_text):\n",
        "    for i in range(MIN_SKIP_PAGES, len(pages_text)):\n",
        "        if match_any(pages_text[i][:2000], INTRO_PATTERNS):\n",
        "            return i\n",
        "    return MAX_SKIP_PAGES_FALLBACK\n",
        "\n",
        "def smart_extract_window(pages_text):\n",
        "    N = len(pages_text)\n",
        "    start_idx = min(first_intro_page(pages_text), N-1)\n",
        "    selected, skipping = [], False\n",
        "\n",
        "    i = start_idx\n",
        "    while i < N - TAIL_EXCLUDE_PAGES:\n",
        "        head = (pages_text[i] or \"\")[:1500]\n",
        "\n",
        "        if match_any(head, STOP_HEADINGS):\n",
        "            # pause extraction at refs\n",
        "            lookahead_text = \" \".join(pages_text[i+1:i+LOOKAHEAD_PAGES+1])\n",
        "            if match_any(lookahead_text, SECTION_HEADINGS):\n",
        "                # skip this refs block, then resume after lookahead\n",
        "                print(f\"Skipped References block at page {i+1}, resuming later…\")\n",
        "                i += LOOKAHEAD_PAGES\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"Final stop at References block on page {i+1}\")\n",
        "                break\n",
        "\n",
        "        selected.append((i, pages_text[i]))\n",
        "        i += 1\n",
        "\n",
        "    return selected, start_idx\n",
        "\n",
        "# === Run ===\n",
        "pages_text = read_pdf_pages_text(PDF_PATH)\n",
        "N = len(pages_text)\n",
        "print(f\"Loaded {N} pages from: {PDF_PATH}\")\n",
        "\n",
        "selected_pairs, start_idx = smart_extract_window(pages_text)\n",
        "selected_pages_text = [t for _, t in selected_pairs]\n",
        "selected_indices = [i for i, _ in selected_pairs]\n",
        "\n",
        "print(\"\\n=== SMART WINDOW SUMMARY ===\")\n",
        "print(f\"Total pages: {N}\")\n",
        "print(f\"Selected pages: {len(selected_pages_text)}\")\n",
        "print(f\"From page {selected_indices[0]+1} to {selected_indices[-1]+1}\")\n",
        "\n",
        "# Peek at first and last selected page\n",
        "print(\"\\n--- Start page preview ---\")\n",
        "print(selected_pages_text[0][:500].replace(\"\\n\", \" \"))\n",
        "print(\"\\n--- End page preview ---\")\n",
        "print(selected_pages_text[-1][:500].replace(\"\\n\", \" \"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What the outputs are:\n",
        "\n",
        "selected_pages_text → the clean list of 234 page texts (from page 19 through 252), excluding chapter-level references and the final back-matter references.\n",
        "\n",
        "selected_page_offset → the original starting page index (18), so page numbers can always be re-attached correctly for provenance.\n",
        "\n",
        "Preview logs → console prints confirming the chosen start page (Introduction, page 19) and end page (before the last references, page 252), with short text snippets for sanity checking."
      ],
      "metadata": {
        "id": "oECQR3KjxqiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 (Block Extraction: sentences, tables, captions)"
      ],
      "metadata": {
        "id": "GFIKKtDl1xJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What the code is doing (3 points):\n",
        "\n",
        "Splits each page into smaller chunks (paragraphs/lines) and classifies them as sentence, table_row, or caption.\n",
        "\n",
        "Sentence tokenization: if a chunk is narrative text, it’s further split into atomic sentences.\n",
        "\n",
        "Keeps provenance: every block is tagged with page_num and block_id for traceability."
      ],
      "metadata": {
        "id": "95VzDTnz3h4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip -q install nltk\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"punkt_tab\", quiet=True) # Add this line\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "def detect_block_type(text):\n",
        "    \"\"\"Heuristic block classifier\"\"\"\n",
        "    # Table-like if many numbers or tab spacing\n",
        "    if re.search(r\"\\d\", text) and (text.count(\" \") > 10 or \"\\t\" in text):\n",
        "        return \"table_row\"\n",
        "    # Caption if starts with Figure/Table\n",
        "    if re.match(r\"^\\s*(figure|fig\\.|table)\\s+\\d+\", text, re.I):\n",
        "        return \"caption\"\n",
        "    return \"sentence\"\n",
        "\n",
        "def extract_blocks(pages_text, page_indices):\n",
        "    blocks = []\n",
        "    for local_idx, page_text in enumerate(pages_text):\n",
        "        page_num = page_indices[local_idx] + 1  # human page number\n",
        "        # Split page into paragraphs/lines\n",
        "        chunks = [c.strip() for c in page_text.split(\"\\n\") if c.strip()]\n",
        "        block_id = 0\n",
        "        for chunk in chunks:\n",
        "            block_type = detect_block_type(chunk)\n",
        "            # If sentence mode: split further\n",
        "            if block_type == \"sentence\":\n",
        "                sentences = sent_tokenize(chunk)\n",
        "                for sent in sentences:\n",
        "                    blocks.append({\n",
        "                        \"page_num\": page_num,\n",
        "                        \"block_id\": f\"{page_num}-{block_id}\",\n",
        "                        \"block_type\": \"sentence\",\n",
        "                        \"text\": sent\n",
        "                    })\n",
        "                    block_id += 1\n",
        "            else:\n",
        "                blocks.append({\n",
        "                    \"page_num\": page_num,\n",
        "                    \"block_id\": f\"{page_num}-{block_id}\",\n",
        "                    \"block_type\": block_type,\n",
        "                    \"text\": chunk\n",
        "                })\n",
        "                block_id += 1\n",
        "    return pd.DataFrame(blocks)\n",
        "\n",
        "# Run block extraction\n",
        "df_blocks = extract_blocks(selected_pages_text, selected_indices)\n",
        "\n",
        "print(f\"Extracted {len(df_blocks)} blocks\")\n",
        "display(df_blocks.head(10)) # Use display for better output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "ko6TMg321sTA",
        "outputId": "0c5f58d5-53a4-4782-8f1b-fae733039bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 5825 blocks\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   page_num block_id block_type  \\\n",
              "0        19     19-0   sentence   \n",
              "1        19     19-1   sentence   \n",
              "2        19     19-2   sentence   \n",
              "3        19     19-3   sentence   \n",
              "4        19     19-4   sentence   \n",
              "5        19     19-5   sentence   \n",
              "6        19     19-6   sentence   \n",
              "7        19     19-7   sentence   \n",
              "8        19     19-8  table_row   \n",
              "9        19     19-9   sentence   \n",
              "\n",
              "                                                text  \n",
              "0                                                  1  \n",
              "1                                         Chapter I.  \n",
              "2                                       Introduction  \n",
              "3                                                 1.  \n",
              "4                        Preamble and knowledge gaps  \n",
              "5                             Paleoproterozoic (i.e.  \n",
              "6  Biri mian) volcano-plutonic belts and sediment...  \n",
              "7  of West Africa not only  provide a complete re...  \n",
              "8  number of world-class gold deposits (Abouc ham...  \n",
              "9  date, a large number of studies have focused o...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-69234ade-1627-4bb1-9633-32559dbc61af\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_num</th>\n",
              "      <th>block_id</th>\n",
              "      <th>block_type</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>19-0</td>\n",
              "      <td>sentence</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19</td>\n",
              "      <td>19-1</td>\n",
              "      <td>sentence</td>\n",
              "      <td>Chapter I.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19</td>\n",
              "      <td>19-2</td>\n",
              "      <td>sentence</td>\n",
              "      <td>Introduction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>19</td>\n",
              "      <td>19-3</td>\n",
              "      <td>sentence</td>\n",
              "      <td>1.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19</td>\n",
              "      <td>19-4</td>\n",
              "      <td>sentence</td>\n",
              "      <td>Preamble and knowledge gaps</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>19</td>\n",
              "      <td>19-5</td>\n",
              "      <td>sentence</td>\n",
              "      <td>Paleoproterozoic (i.e.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>19</td>\n",
              "      <td>19-6</td>\n",
              "      <td>sentence</td>\n",
              "      <td>Biri mian) volcano-plutonic belts and sediment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>19</td>\n",
              "      <td>19-7</td>\n",
              "      <td>sentence</td>\n",
              "      <td>of West Africa not only  provide a complete re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>19</td>\n",
              "      <td>19-8</td>\n",
              "      <td>table_row</td>\n",
              "      <td>number of world-class gold deposits (Abouc ham...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>19</td>\n",
              "      <td>19-9</td>\n",
              "      <td>sentence</td>\n",
              "      <td>date, a large number of studies have focused o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69234ade-1627-4bb1-9633-32559dbc61af')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-69234ade-1627-4bb1-9633-32559dbc61af button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-69234ade-1627-4bb1-9633-32559dbc61af');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-22c8af5e-4f7a-407c-aa9c-7477ea2e7dba\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-22c8af5e-4f7a-407c-aa9c-7477ea2e7dba')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-22c8af5e-4f7a-407c-aa9c-7477ea2e7dba button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df_blocks\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"page_num\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 19,\n        \"max\": 19,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          19\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"block_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"19-8\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"block_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"table_row\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"number of world-class gold deposits (Abouc hami et al., 1990; Boher et al., 1992). To\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "output:\n",
        "\n",
        "Split all 234 pages into 5825 atomic blocks (sentences, table-like rows, and captions).\n",
        "\n",
        "Classified each block into sentence, table_row, or caption using simple heuristics.\n",
        "\n",
        "Attached provenance (page_num, block_id) so each block can be traced back to its source page.\n",
        "\n",
        "📄 What the output is:\n",
        "\n",
        "df_blocks: a DataFrame with 4 key columns — page_num, block_id, block_type, and text.\n",
        "\n",
        "First rows confirm: the start page (19) has its chapter heading, intro sentences, and even a table-like line flagged correctly."
      ],
      "metadata": {
        "id": "yy3R4NUs4wTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tep 3: Dictionary & Regex Pre-Filter"
      ],
      "metadata": {
        "id": "jDY9a1QZ4zBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this code does (3 points)\n",
        "\n",
        "Loads a ready-to-use DICT (no commented placeholders) with core geo terms, units, and aliases/normalisers.\n",
        "\n",
        "Scans each block for (a) at least one domain keyword and (b) quantitative patterns (numbers + units, ranges, ±error, oxide %).\n",
        "\n",
        "Emits a shortlisted DataFrame with provenance + parsed numbers, and saves it to /content/candidate_blocks_step3.csv."
      ],
      "metadata": {
        "id": "zDYeSu-r6SIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip -q install pandas rapidfuzz\n",
        "\n",
        "import re, json\n",
        "from collections import Counter\n",
        "from rapidfuzz import process, fuzz\n",
        "import pandas as pd\n",
        "\n",
        "# A) Dictionary\n",
        "DICT = {\n",
        "    \"rock_type\": [\n",
        "        \"limestone\",\"dolomite\",\"dolostone\",\"wacke\",\"arenite\",\"siltstone\",\"argillite\",\n",
        "        \"mudstone\",\"shale\",\"sandstone\",\"conglomerate\",\"breccia\",\"granite\",\"monzogranite\",\n",
        "        \"granodiorite\",\"diorite\",\"tonalite\",\"gabbro\",\"basalt\",\"andesite\",\"rhyolite\",\n",
        "        \"komatiite\",\"trachyte\",\"phonolite\",\"gneiss\",\"schist\",\"quartzite\"\n",
        "    ],\n",
        "    \"minerals\": [\n",
        "        \"pyrite\",\"arsenopyrite\",\"chalcopyrite\",\"pyrrhotite\",\"sphalerite\",\"galena\",\n",
        "        \"magnetite\",\"hematite\",\"native gold\",\"electrum\",\"muscovite\",\"biotite\",\n",
        "        \"chlorite\",\"sericite\",\"albite\",\"epidote\",\"carbonate\",\"quartz\",\"feldspar\"\n",
        "    ],\n",
        "    \"mineralisation\": [\n",
        "        \"sulfide\",\"sulphide\",\"stockwork\",\"vein\",\"disseminated\",\"massive\",\"breccia\",\n",
        "        \"replacement\",\"porphyry\",\"orogenic\"\n",
        "    ],\n",
        "    \"tectonism_event\": [\n",
        "        \"D1\",\"D2\",\"D3\",\"D4\",\"compressional\",\"extensional\",\"transcurrent\",\n",
        "        \"sinistral\",\"dextral\",\"thrust\",\"fold\",\"shear zone\",\"shear\"\n",
        "    ],\n",
        "    \"structures\": [\n",
        "        \"foliation\",\"lineation\",\"cleavage\",\"vein\",\"veinlet\",\"breccia\",\"schistosity\",\n",
        "        \"plunge\",\"strike\",\"dip\",\"bedding\",\"lamination\",\"fault\"\n",
        "    ],\n",
        "    \"methods\": [\n",
        "        \"ICP-MS\",\"LA-ICP-MS\",\"EPMA\",\"SEM\",\"XRD\",\"XRF\",\"AAS\",\"fire assay\",\n",
        "        \"TIMS\",\"ID-TIMS\",\"MC-ICP-MS\",\"SIMS\",\"LAICPMS\",\"microprobe\"\n",
        "    ],\n",
        "    \"geochronology_terms\": [\n",
        "        \"zircon\",\"monazite\",\"baddeleyite\",\"concordia\",\"intercept\",\"weighted mean\",\n",
        "        \"MSWD\",\"discordant\",\"206Pb/238U\",\"207Pb/206Pb\",\"U–Pb\",\"U-Pb\",\"age\",\"dated\"\n",
        "    ],\n",
        "    \"assay_elements\": [\n",
        "        \"Au\",\"Ag\",\"As\",\"Sb\",\"Cu\",\"Pb\",\"Zn\",\"Ni\",\"Co\",\"Fe\",\"S\",\n",
        "        \"SiO2\",\"Al2O3\",\"MgO\",\"CaO\",\"K2O\",\"Na2O\",\"TiO2\",\"P2O5\",\"LOI\",\"Cr2O3\",\"MnO\"\n",
        "    ],\n",
        "    \"units\": [\"ppm\",\"ppb\",\"wt%\",\"%\",\"g/t\",\"mg/kg\",\"µg/g\",\"ug/g\",\"Ma\",\"Ga\",\"°C\",\"deg C\"],\n",
        "    \"stopwords_geo\": [\"references\",\"bibliography\",\"acknowledgements\",\"appendix\"]\n",
        "}\n",
        "\n",
        "UNIT_NORMALISE = {\"gpt\": \"g/t\", \"percent\": \"%\", \"ug/g\": \"µg/g\", \"deg c\": \"°C\"}\n",
        "ALIAS_MAP = {\n",
        "    \"gold\":\"Au\",\"arsenic\":\"As\",\"antimony\":\"Sb\",\"copper\":\"Cu\",\"lead\":\"Pb\",\"zinc\":\"Zn\",\n",
        "    \"nickel\":\"Ni\",\"cobalt\":\"Co\",\"sulfur\":\"S\",\"sulphur\":\"S\"\n",
        "}\n",
        "\n",
        "def normalise_unit(u: str|None) -> str|None:\n",
        "    if not u: return None\n",
        "    u = u.strip().lower()\n",
        "    return UNIT_NORMALISE.get(u, u).replace(\"ug/g\",\"µg/g\")\n",
        "\n",
        "def alias_text(text: str) -> str:\n",
        "    low = text.lower()\n",
        "    for k, v in ALIAS_MAP.items():\n",
        "        low = re.sub(rf\"\\b{k}\\b\", v.lower(), low)\n",
        "    return low\n",
        "\n",
        "# B) Numeric/Unit regex\n",
        "NUM = r\"\\d+(?:[\\.,]\\d+)?\"\n",
        "RANGE_SEP = r\"(?:–|-|to)\"\n",
        "PATTERNS = {\n",
        "    \"num_unit\": re.compile(rf\"(?P<val>{NUM})\\s*(?P<unit>ppm|ppb|wt%|%|g\\/t|mg\\/kg|µg\\/g|ug\\/g|Ma|Ga|°C)\\b\", re.I),\n",
        "    \"range\": re.compile(rf\"(?P<v1>{NUM})\\s*{RANGE_SEP}\\s*(?P<v2>{NUM})\\s*(?P<unit>ppm|ppb|%|g\\/t|Ma|Ga)?\\b\", re.I),\n",
        "    \"between_age\": re.compile(rf\"\\bbetween\\s+(?P<v1>{NUM})\\s*(?:–|-|to|and)\\s*(?P<v2>{NUM})\\s*(?P<unit>Ma|Ga)\\b\", re.I),\n",
        "    \"plusminus\": re.compile(rf\"(?P<mean>{NUM})\\s*(?:±|\\+\\/-)\\s*(?P<err>{NUM})\\s*(?P<unit>Ma|g\\/t|ppm|%|°C)?\\b\", re.I),\n",
        "    \"oxide_pct\": re.compile(rf\"(?P<oxide>[A-Z][a-z]?(?:\\d)?O\\d?)\\s*(?P<val>{NUM})\\s*%\", re.I),\n",
        "}\n",
        "\n",
        "# C) Noise guards & context checks\n",
        "MONTHS = r\"(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|january|february|march|april|june|july|august|september|october|november|december)\"\n",
        "CITATION_CUES = r\"\\b(journal|geological society|special publications|bulletin|v\\.|vol\\.|issue|no\\.|pp?\\.|doi:|issn|isbn|proceedings|symposium|conference|abstracts|et al\\.)\\b\"\n",
        "\n",
        "RANGE_CONTEXT_OK = [\n",
        "    \"ma\",\"ga\",\"age\",\"dated\",\"u–pb\",\"u-pb\",\"zircon\",\"monazite\",\"concordia\",\"intercept\",\n",
        "    \"ppm\",\"ppb\",\"g/t\",\"wt%\",\"%\",\"oxide\",\"sio2\",\"feo\",\"tio2\",\"mgo\",\"cao\",\"k2o\",\"na2o\",\"p2o5\",\n",
        "    \"mg#\",\"mswd\"\n",
        "]\n",
        "\n",
        "def has_range_context(text: str) -> bool:\n",
        "    low = text.lower()\n",
        "    return any(tok in low for tok in RANGE_CONTEXT_OK)\n",
        "\n",
        "def looks_like_date_or_citation(text: str) -> bool:\n",
        "    low = text.lower()\n",
        "    if re.search(MONTHS, low) and re.search(r\"\\b\\d{1,2}\\s*(?:–|-|to)\\s*\\d{1,2}\\b\", low):\n",
        "        return True\n",
        "    if re.search(CITATION_CUES, low) and re.search(r\"\\b\\d{2,6}\\s*(?:–|-|to)\\s*\\d{2,6}\\b\", low):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def age_signal_present(text: str) -> bool:\n",
        "    low = text.lower()\n",
        "    return any(tok in low for tok in [\"u–pb\",\"u-pb\",\"concordia\",\"mswd\",\"age\",\"dated\"])\n",
        "\n",
        "# D) Candidate finder over df_blocks\n",
        "def find_candidates_from_blocks(df_blocks, min_keywords=1, require_numeric=True, fuzzy_cutoff=95):\n",
        "    recs = []\n",
        "    for _, row in df_blocks.iterrows():\n",
        "        raw_text = str(row[\"text\"]).strip()\n",
        "        if not raw_text:\n",
        "            continue\n",
        "\n",
        "        low = alias_text(raw_text)\n",
        "\n",
        "        # Stronger citation/date guard: allow only if explicit unit/± OR clear age signal\n",
        "        if looks_like_date_or_citation(raw_text):\n",
        "            if not (PATTERNS[\"num_unit\"].search(raw_text) or PATTERNS[\"plusminus\"].search(raw_text) or age_signal_present(raw_text)):\n",
        "                continue\n",
        "\n",
        "        # Keyword categories\n",
        "        keyword_hits = set()\n",
        "        for cat, terms in DICT.items():\n",
        "            if cat in (\"units\",\"stopwords_geo\"):\n",
        "                continue\n",
        "            if any(t.lower() in low for t in terms):\n",
        "                keyword_hits.add(cat); continue\n",
        "            best = process.extractOne(low, terms, scorer=fuzz.partial_ratio, score_cutoff=fuzzy_cutoff)\n",
        "            if best: keyword_hits.add(cat)\n",
        "\n",
        "        if len(keyword_hits) < min_keywords:\n",
        "            continue\n",
        "\n",
        "        # Quantitative patterns\n",
        "        m_numunit   = list(PATTERNS[\"num_unit\"].finditer(raw_text))\n",
        "        m_pm        = list(PATTERNS[\"plusminus\"].finditer(raw_text))\n",
        "        m_oxide     = list(PATTERNS[\"oxide_pct\"].finditer(raw_text))\n",
        "        m_range     = list(PATTERNS[\"range\"].finditer(raw_text))\n",
        "        m_between   = list(PATTERNS[\"between_age\"].finditer(raw_text))\n",
        "\n",
        "        explicit_ok = bool(m_numunit or m_pm or m_oxide)\n",
        "        valid_numeric = explicit_ok\n",
        "\n",
        "        # Allow ranges only with context; require stronger geo signal if still unit-less\n",
        "        if not explicit_ok:\n",
        "            if (m_range or m_between) and has_range_context(raw_text):\n",
        "                valid_numeric = len(keyword_hits) >= 2\n",
        "            else:\n",
        "                valid_numeric = False\n",
        "\n",
        "        # Co-occurrence rule for Ma/Ga without ± :\n",
        "        if explicit_ok and not m_pm:\n",
        "            # if ONLY Ma/Ga units found (no ppm/%/g/t/°C), demand geochron signal or ≥2 categories\n",
        "            only_age_units = all((m.group(\"unit\") or \"\").lower() in [\"ma\",\"ga\"] for m in m_numunit) if m_numunit else False\n",
        "            if only_age_units and not ((\"geochronology_terms\" in keyword_hits) or (len(keyword_hits) >= 2)):\n",
        "                valid_numeric = False\n",
        "\n",
        "        if require_numeric and not valid_numeric:\n",
        "            continue\n",
        "\n",
        "        # Parse numbers\n",
        "        numbers = []\n",
        "        for m in m_numunit:\n",
        "            numbers.append({\n",
        "                \"type\":\"num_unit\",\n",
        "                \"value\": float(m.group(\"val\").replace(\",\", \".\")),\n",
        "                \"unit\":  normalise_unit(m.group(\"unit\"))\n",
        "            })\n",
        "        for m in m_pm:\n",
        "            numbers.append({\n",
        "                \"type\":\"plusminus\",\n",
        "                \"mean\":  float(m.group(\"mean\").replace(\",\", \".\")),\n",
        "                \"error\": float(m.group(\"err\").replace(\",\", \".\")),\n",
        "                \"unit\":  normalise_unit(m.group(\"unit\"))\n",
        "            })\n",
        "        for m in m_oxide:\n",
        "            numbers.append({\n",
        "                \"type\":\"oxide_pct\",\n",
        "                \"oxide\": m.group(\"oxide\"),\n",
        "                \"value\": float(m.group(\"val\").replace(\",\", \".\")),\n",
        "                \"unit\":  \"%\"\n",
        "            })\n",
        "        # Age ranges\n",
        "        if m_between:\n",
        "            for m in m_between:\n",
        "                numbers.append({\n",
        "                    \"type\":\"range\",\n",
        "                    \"value_min\": float(m.group(\"v1\").replace(\",\", \".\")),\n",
        "                    \"value_max\": float(m.group(\"v2\").replace(\",\", \".\")),\n",
        "                    \"unit\":      normalise_unit(m.group(\"unit\"))\n",
        "                })\n",
        "        elif m_range and has_range_context(raw_text) and len(keyword_hits) >= 2:\n",
        "            for m in m_range:\n",
        "                numbers.append({\n",
        "                    \"type\":\"range\",\n",
        "                    \"value_min\": float(m.group(\"v1\").replace(\",\", \".\")),\n",
        "                    \"value_max\": float(m.group(\"v2\").replace(\",\", \".\")),\n",
        "                    \"unit\":      normalise_unit(m.group(\"unit\")) if m.group(\"unit\") else None\n",
        "                })\n",
        "\n",
        "        if not numbers:\n",
        "            continue\n",
        "\n",
        "        recs.append({\n",
        "            \"page_num\": row[\"page_num\"],\n",
        "            \"block_id\": row[\"block_id\"],\n",
        "            \"block_type\": row[\"block_type\"],\n",
        "            \"text\": raw_text,\n",
        "            \"keyword_categories\": sorted(list(keyword_hits)),\n",
        "            \"numbers_json\": json.dumps(numbers, ensure_ascii=False),\n",
        "            \"has_numbers\": True\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame.from_records(recs)\n",
        "\n",
        "#  E) Run and save\n",
        "df_cands = find_candidates_from_blocks(df_blocks, min_keywords=1, require_numeric=True)\n",
        "\n",
        "df_cands = df_cands.sort_values([\"page_num\",\"block_id\"]).reset_index(drop=True)\n",
        "print(f\"Candidate blocks found: {len(df_cands)}\")\n",
        "\n",
        "display_cols = [\"page_num\",\"block_id\",\"block_type\",\"keyword_categories\",\"text\",\"numbers_json\"]\n",
        "print(df_cands[display_cols].head(12).to_string(index=False)[:2500])\n",
        "\n",
        "OUT_PATH = \"/content/candidate_blocks_step3.csv\"\n",
        "df_cands.to_csv(OUT_PATH, index=False)\n",
        "print(f\"\\nSaved candidates → {OUT_PATH}\")\n",
        "\n",
        "# Quick category stats\n",
        "cat_counts = Counter()\n",
        "for cats in df_cands[\"keyword_categories\"]:\n",
        "    for c in cats:\n",
        "        cat_counts[c] += 1\n",
        "print(\"\\nTop categories (rough):\")\n",
        "for c, n in cat_counts.most_common(12):\n",
        "    print(f\"  {c:22s} {n}\")\n"
      ],
      "metadata": {
        "id": "f96CSYGh6Wk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc00b40-321e-4217-8ebe-0e11e0bace87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Candidate blocks found: 174\n",
            " page_num block_id block_type                               keyword_categories                                                                                    text                                                                                                                                                                                                                               numbers_json\n",
            "       19    19-18  table_row                                 [assay_elements]  2158 Ma gold at Wassa, Parra-Avila, in press; 2105 ± 2 Ma gold at Ashanti, Oberthür et                                                                 [{\"type\": \"num_unit\", \"value\": 2158.0, \"unit\": \"ma\"}, {\"type\": \"num_unit\", \"value\": 2.0, \"unit\": \"ma\"}, {\"type\": \"plusminus\", \"mean\": 2105.0, \"error\": 2.0, \"unit\": \"ma\"}]\n",
            "       19    19-19  table_row                                 [assay_elements]     al., 1998; 2063 ± 9 Ma gold at Damang, Pigois et al., 2003). Nevertheless, the vast                                                                                                                      [{\"type\": \"num_unit\", \"value\": 9.0, \"unit\": \"ma\"}, {\"type\": \"plusminus\", \"mean\": 2063.0, \"error\": 9.0, \"unit\": \"ma\"}]\n",
            "       19    19-23  table_row            [assay_elements, geochronology_terms]           Eburnean orogeny dated between 2115 and 2060 Ma (e.g., Feybesse et al., 2006;                                                                                                           [{\"type\": \"num_unit\", \"value\": 2060.0, \"unit\": \"ma\"}, {\"type\": \"range\", \"value_min\": 2115.0, \"value_max\": 2060.0, \"unit\": \"ma\"}]\n",
            "       42    42-18  table_row [assay_elements, geochronology_terms, rock_type]      dated between 2160 ± 16 Ma (Boher et al., 199 2; Sm-Nd on whole rock andesite) and                                                                                                                    [{\"type\": \"num_unit\", \"value\": 16.0, \"unit\": \"ma\"}, {\"type\": \"plusminus\", \"mean\": 2160.0, \"error\": 16.0, \"unit\": \"ma\"}]\n",
            "       42    42-19  table_row                      [assay_elements, rock_type]       2197 ± 13 Ma (Dia, 1988; Sm-Nd on whole rock  basalt). The Mako crustal rocks are                                                                                                                    [{\"type\": \"num_unit\", \"value\": 13.0, \"unit\": \"ma\"}, {\"type\": \"plusminus\", \"mean\": 2197.0, \"error\": 13.0, \"unit\": \"ma\"}]\n",
            "       42    42-21  table_row [assay_elements, geochronology_terms, rock_type]       Mam\n",
            "\n",
            "Saved candidates → /content/candidate_blocks_step3.csv\n",
            "\n",
            "Top categories (rough):\n",
            "  assay_elements         173\n",
            "  geochronology_terms    94\n",
            "  minerals               30\n",
            "  rock_type              21\n",
            "  mineralisation         10\n",
            "  methods                6\n",
            "  tectonism_event        4\n",
            "  structures             3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: LLM Structured Extraction.\n"
      ],
      "metadata": {
        "id": "C04RxUbRDA2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this code will do (3 points)\n",
        "\n",
        "Calls an LLM on your shortlisted df_cands blocks to extract strict JSON facts with categories, values, units, and provenance.\n",
        "\n",
        "Validates/normalises the JSON (schema checks, unit normalisation, numeric coercion) and assigns a confidence.\n",
        "\n",
        "Writes tidy outputs: a JSONL of raw model facts and a flattened CSV/Parquet ready for analysis.\n",
        "\n",
        "⚙️ You’ll need an OpenAI-compatible API key in Colab:\n",
        "import os; os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
      ],
      "metadata": {
        "id": "-5qFLL_6D-AW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4) LLM Structured Extraction\n",
        "!pip -q install --no-cache-dir pandas pydantic requests\n",
        "\n",
        "import os, json, time, re, getpass, requests\n",
        "import pandas as pd\n",
        "from typing import List, Optional, Dict, Any\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "\n",
        "\n",
        "OPENAI_KEY = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    OPENAI_KEY = userdata.get(\"sandra\")\n",
        "except Exception:\n",
        "    OPENAI_KEY = None\n",
        "\n",
        "if not OPENAI_KEY:\n",
        "    OPENAI_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "if not OPENAI_KEY:\n",
        "    print(\"Paste your OpenAI API key (hidden):\")\n",
        "    OPENAI_KEY = getpass.getpass(\"> \").strip()\n",
        "if not OPENAI_KEY:\n",
        "    raise RuntimeError(\"No API key provided (secret 'adhi.key' not found).\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_KEY\n",
        "\n",
        "\n",
        "API_URL     = \"https://api.openai.com/v1/chat/completions\"\n",
        "MODEL_NAME  = \"gpt-4o-mini\"\n",
        "BATCH_SIZE  = 20\n",
        "MAX_BLOCKS  = None\n",
        "TEMPERATURE = 0\n",
        "RETRY       = 3\n",
        "SLEEP_BETWEEN = 1.25\n",
        "\n",
        "ALLOWED_CATEGORIES = [\n",
        "    \"lithology\",\"rock_type\",\"minerals\",\n",
        "    \"mineralisation\",\"tectonism_event\",\"structures\",\n",
        "    \"assay\",\"geochemistry\",\"geochronology\",\"magmatism\",\n",
        "    \"methods\",\"stratigraphy\"\n",
        "]\n",
        "ALLOWED_UNITS = [\"ppm\",\"ppb\",\"wt%\",\"%\",\"g/t\",\"mg/kg\",\"µg/g\",\"Ma\",\"Ga\",\"°C\"]\n",
        "\n",
        "UNIT_NORMALISE = {\"gpt\": \"g/t\", \"percent\": \"%\", \"ug/g\": \"µg/g\", \"deg c\": \"°C\"}\n",
        "def normalise_unit(u: Optional[str]) -> Optional[str]:\n",
        "    if not u: return None\n",
        "    u = u.strip()\n",
        "    lu = UNIT_NORMALISE.get(u.lower(), u.lower())\n",
        "    return {\"ma\":\"Ma\",\"ga\":\"Ga\",\"°c\":\"°C\"}.get(lu, lu)\n",
        "\n",
        "# C) Output schema\n",
        "class Fact(BaseModel):\n",
        "    category: str = Field(..., description=f\"One of: {ALLOWED_CATEGORIES}\")\n",
        "    entity: Optional[str] = None\n",
        "    attribute: str = Field(..., description=\"e.g., Au, As, SiO2, age, MSWD, grade, orientation\")\n",
        "    value: Optional[float] = None\n",
        "    unit: Optional[str] = None\n",
        "    value_min: Optional[float] = None\n",
        "    value_max: Optional[float] = None\n",
        "    error: Optional[float] = None\n",
        "    error_unit: Optional[str] = None\n",
        "    method: Optional[str] = None\n",
        "    mineral_phase: Optional[str] = None\n",
        "    sample_id: Optional[str] = None\n",
        "    context_snippet: str = Field(..., max_length=240)\n",
        "    page: int = Field(..., description=\"1-based page number\")\n",
        "    block_id: str = Field(..., description=\"Step-2 block id\")\n",
        "    confidence: float = Field(..., ge=0.0, le=1.0)\n",
        "\n",
        "class BatchOut(BaseModel):\n",
        "    results: list[Fact] = Field(default_factory=list)\n",
        "\n",
        "#D) Prompts\n",
        "SYSTEM_PROMPT = \"\"\"You extract structured geology facts from thesis text.\n",
        "Return STRICT JSON matching the schema. Do not add keys.\n",
        "Prefer atomic facts (split multi-values). Provide <=240 char context_snippet.\n",
        "If no measurable geological quantity exists, return results: [].\"\"\"\n",
        "\n",
        "USER_TEMPLATE = \"\"\"You will be given a list of blocks from a geology thesis with page numbers and block_ids.\n",
        "For each block, extract zero or more atomic facts as JSON objects with this schema:\n",
        "\n",
        "Fact: {{\n",
        "  \"category\": one of [\"lithology\",\"rock_type\",\"minerals\",\"mineralisation\",\"tectonism_event\",\"structures\",\"assay\",\"geochemistry\",\"geochronology\",\"magmatism\",\"methods\",\"stratigraphy\"],\n",
        "  \"entity\": optional string,\n",
        "  \"attribute\": string (e.g., Au, As, SiO2, age, MSWD, grade, orientation),\n",
        "  \"value\": number (if scalar),\n",
        "  \"unit\": optional string in [\"ppm\",\"ppb\",\"wt%\",\"%\",\"g/t\",\"mg/kg\",\"µg/g\",\"Ma\",\"Ga\",\"°C\"],\n",
        "  \"value_min\": number (if a range),\n",
        "  \"value_max\": number (if a range),\n",
        "  \"error\": number (if ± is present),\n",
        "  \"error_unit\": optional string (often same as unit),\n",
        "  \"method\": optional string (e.g., U–Pb, LA-ICP-MS, EPMA, Fire Assay),\n",
        "  \"mineral_phase\": optional string (e.g., zircon, monazite),\n",
        "  \"sample_id\": optional string,\n",
        "  \"context_snippet\": string (<=240 chars),\n",
        "  \"page\": integer (1-based),\n",
        "  \"block_id\": string,\n",
        "  \"confidence\": float between 0 and 1\n",
        "}}\n",
        "\n",
        "Rules:\n",
        "- Use \"geochronology\" + attribute \"age\" for Ma/Ga ages; set 'error' if ±; set 'value_min/value_max' for ranges like \"between 2115 and 2060 Ma\".\n",
        "- Use \"assay\" for element grades (Au, As, Zn...) with g/t, ppm, %, etc.; use \"geochemistry\" for oxides like SiO2, Al2O3.\n",
        "- Do not invent sample IDs or methods; include only if present or clearly implied.\n",
        "- Ignore citation-only lines.\n",
        "\n",
        "Return strictly: {{\"results\":[...]}}.\n",
        "\n",
        "Blocks:\n",
        "{blocks_json}\n",
        "\"\"\"\n",
        "\n",
        "#Payload builder\n",
        "def make_blocks_payload(df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
        "    page_col = \"page\" if \"page\" in df.columns else \"page_num\"\n",
        "    return [\n",
        "        {\"page\": int(row[page_col]), \"block_id\": str(row[\"block_id\"]), \"text\": str(row[\"text\"])}\n",
        "        for _, row in df.iterrows()\n",
        "    ]\n",
        "\n",
        "# F) LLM call (direct REST)\n",
        "def call_llm(blocks_chunk: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    user = USER_TEMPLATE.format(blocks_json=json.dumps(blocks_chunk, ensure_ascii=False))\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    body = {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"temperature\": TEMPERATURE,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\",   \"content\": user},\n",
        "        ],\n",
        "        \"response_format\": {\"type\": \"json_object\"},\n",
        "        \"max_tokens\": 2048,\n",
        "    }\n",
        "    r = requests.post(API_URL, headers=headers, json=body, timeout=60)\n",
        "    if r.status_code != 200:\n",
        "        raise RuntimeError(f\"OpenAI API error {r.status_code}: {r.text[:500]}\")\n",
        "    data = r.json()\n",
        "    txt = data[\"choices\"][0][\"message\"][\"content\"]\n",
        "    # Strip optional code fences if any\n",
        "    if txt and txt.strip().startswith(\"```\"):\n",
        "        t = txt.strip().strip(\"`\")\n",
        "        if t.lower().startswith(\"json\"):\n",
        "            t = t[4:].strip()\n",
        "        txt = t\n",
        "    return json.loads(txt)\n",
        "\n",
        "# G) Batch runner\n",
        "def run_batches(df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
        "    out = []\n",
        "    n = len(df)\n",
        "    if MAX_BLOCKS is not None:\n",
        "        df = df.head(MAX_BLOCKS); n = len(df)\n",
        "    for i in range(0, n, BATCH_SIZE):\n",
        "        chunk_df = df.iloc[i:i+BATCH_SIZE]\n",
        "        payload = make_blocks_payload(chunk_df)\n",
        "        tries = 0\n",
        "        while True:\n",
        "            try:\n",
        "                out.append(call_llm(payload))\n",
        "                break\n",
        "            except Exception as e:\n",
        "                tries += 1\n",
        "                if tries >= RETRY:\n",
        "                    print(f\"[WARN] batch {i}-{i+len(chunk_df)} failed: {e}\")\n",
        "                    out.append({\"results\":[]})\n",
        "                    break\n",
        "                time.sleep(SLEEP_BETWEEN * tries)\n",
        "    return out\n",
        "\n",
        "#H) Validate & normalise\n",
        "def validate_and_flatten(all_responses: List[Dict[str, Any]]) -> pd.DataFrame:\n",
        "    facts: List[Dict[str, Any]] = []\n",
        "    for resp in all_responses:\n",
        "        results = resp.get(\"results\", [])\n",
        "        if not isinstance(results, list):\n",
        "            continue\n",
        "        for obj in results:\n",
        "            try:\n",
        "                if \"unit\" in obj: obj[\"unit\"] = normalise_unit(obj[\"unit\"])\n",
        "                if \"error_unit\" in obj: obj[\"error_unit\"] = normalise_unit(obj[\"error_unit\"])\n",
        "                fact = Fact(**obj)\n",
        "                facts.append(fact.dict())\n",
        "            except ValidationError:\n",
        "                safe = {k:v for k,v in obj.items() if k in Fact.model_fields}\n",
        "                try:\n",
        "                    if \"unit\" in safe: safe[\"unit\"] = normalise_unit(safe[\"unit\"])\n",
        "                    if \"error_unit\" in safe: safe[\"error_unit\"] = normalise_unit(safe[\"error_unit\"])\n",
        "                    fact = Fact(**safe)\n",
        "                    facts.append(fact.dict())\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    if not facts:\n",
        "        return pd.DataFrame(columns=list(Fact.model_fields.keys()))\n",
        "\n",
        "    df = pd.DataFrame(facts)\n",
        "    df[\"confidence\"] = df[\"confidence\"].clip(0.0, 1.0).fillna(0.5)\n",
        "    df[\"category\"] = df[\"category\"].str.strip().str.lower().map({\n",
        "        \"lithology\":\"lithology\",\"rock_type\":\"rock_type\",\"minerals\":\"minerals\",\n",
        "        \"mineralisation\":\"mineralisation\",\"tectonism_event\":\"tectonism_event\",\n",
        "        \"structures\":\"structures\",\"assay\":\"assay\",\"geochemistry\":\"geochemistry\",\n",
        "        \"geochronology\":\"geochronology\",\"magmatism\":\"magmatism\",\"methods\":\"methods\",\n",
        "        \"stratigraphy\":\"stratigraphy\"\n",
        "    }).fillna(\"assay\")\n",
        "\n",
        "    for col in [\"value\",\"value_min\",\"value_max\",\"error\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    return df.sort_values([\"page\",\"block_id\",\"category\",\"attribute\"]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "cands_df = df_cands[[\"page_num\",\"block_id\",\"text\"]]\n",
        "responses = run_batches(cands_df)\n",
        "df_facts   = validate_and_flatten(responses)\n",
        "\n",
        "\n",
        "JSONL_PATH = \"/content/extracted_step4.jsonl\"\n",
        "CSV_PATH   = \"/content/extracted_step4.csv\"\n",
        "PARQ_PATH  = \"/content/extracted_step4.parquet\"\n",
        "\n",
        "with open(JSONL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, r in df_facts.iterrows():\n",
        "        f.write(json.dumps(r.to_dict(), ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "df_facts.to_csv(CSV_PATH, index=False)\n",
        "try:\n",
        "    df_facts.to_parquet(PARQ_PATH, index=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(f\"Saved JSONL → {JSONL_PATH}\")\n",
        "print(f\"Saved CSV   → {CSV_PATH}\")\n",
        "print(f\"Saved Parquet (if available) → {PARQ_PATH}\")\n",
        "print(\"\\nPreview:\")\n",
        "print(df_facts.head(10).to_string(index=False))\n",
        "print(\"\\nCounts by category:\")\n",
        "print(df_facts[\"category\"].value_counts())\n"
      ],
      "metadata": {
        "id": "DxcOuDDwFjcT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7089372e-2f08-45b7-f2c4-af272f44ee49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved JSONL → /content/extracted_step4.jsonl\n",
            "Saved CSV   → /content/extracted_step4.csv\n",
            "Saved Parquet (if available) → /content/extracted_step4.parquet\n",
            "\n",
            "Preview:\n",
            "     category entity attribute  value unit  value_min  value_max  error error_unit method mineral_phase sample_id                                                                        context_snippet  page block_id  confidence\n",
            "geochronology   None       age 2158.0   Ma        NaN        NaN    NaN       None   None          None      None 2158 Ma gold at Wassa, Parra-Avila, in press; 2105 ± 2 Ma gold at Ashanti, Oberthür et    19    19-18         0.9\n",
            "geochronology   None       age 2105.0   Ma        NaN        NaN    2.0       None   None          None      None 2158 Ma gold at Wassa, Parra-Avila, in press; 2105 ± 2 Ma gold at Ashanti, Oberthür et    19    19-18         0.9\n",
            "geochronology   None       age 2063.0   Ma        NaN        NaN    9.0       None   None          None      None               2063 ± 9 Ma gold at Damang, Pigois et al., 2003). Nevertheless, the vast    19    19-19         0.9\n",
            "geochronology   None       age    NaN None     2115.0     2060.0    NaN       None   None          None      None          Eburnean orogeny dated between 2115 and 2060 Ma (e.g., Feybesse et al., 2006;    19    19-23         0.8\n",
            "geochronology   None       age    NaN   Ma     2160.0     2160.0   16.0       None   None          None      None     dated between 2160 ± 16 Ma (Boher et al., 199 2; Sm-Nd on whole rock andesite) and    42    42-18         0.8\n",
            "geochronology   None       age 2197.0   Ma        NaN        NaN   13.0       None   None          None      None      2197 ± 13 Ma (Dia, 1988; Sm-Nd on whole rock  basalt). The Mako crustal rocks are    42    42-19         0.9\n",
            "geochronology   None       age 2074.0   Ma        NaN        NaN    9.0       None   None          None      None      Mamakono granodiorites, respectively dated at 2074 ± 9 Ma and 2076 ± 3 Ma (Bassot    42    42-21         0.9\n",
            "geochronology   None       age 2076.0   Ma        NaN        NaN    3.0       None   None          None      None      Mamakono granodiorites, respectively dated at 2074 ± 9 Ma and 2076 ± 3 Ma (Bassot    42    42-21         0.9\n",
            "geochronology   None       age 2165.0   Ma        NaN        NaN    1.0       None   None          None      None      uniform age of 2165 ± 1 Ma (Hirdes and Da vis, 2002; Pb-Pb zircon) for the source    42    42-33         0.9\n",
            "geochronology   None       age 2079.0   Ma        NaN        NaN    2.0       None   None          None      None      Saraya granite, which has been dated at 2079 ± 2 Ma (Hirdes and Davis, 2002; U-Pb    42    42-36         0.9\n",
            "\n",
            "Counts by category:\n",
            "category\n",
            "geochronology      104\n",
            "assay               13\n",
            "geochemistry        12\n",
            "tectonism_event      7\n",
            "lithology            3\n",
            "mineralisation       2\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-724341226.py:186: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  facts.append(fact.dict())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = df_facts.copy()\n",
        "\n",
        "# fill missing error_unit when unit exists and error is present\n",
        "mask_err_no_unit = df[\"error\"].notna() & df[\"unit\"].notna() & df[\"error_unit\"].isna()\n",
        "df.loc[mask_err_no_unit, \"error_unit\"] = df.loc[mask_err_no_unit, \"unit\"]\n",
        "\n",
        "# infer unit for ranges from context\n",
        "def infer_unit_from_context(snippet, current):\n",
        "    if current: return current\n",
        "    s = (snippet or \"\").lower()\n",
        "    if \" ma\" in s: return \"Ma\"\n",
        "    if \" ga\" in s: return \"Ga\"\n",
        "    if \" g/t\" in s: return \"g/t\"\n",
        "    if \" ppm\" in s: return \"ppm\"\n",
        "    if \" wt%\" in s or \" wt %\" in s or \" %\" in s: return \"%\"\n",
        "    return None\n",
        "\n",
        "rng = df[\"value\"].isna() & df[\"value_min\"].notna() & df[\"value_max\"].notna()\n",
        "df.loc[rng, \"unit\"] = [\n",
        "    infer_unit_from_context(sn, u) for sn, u in zip(df.loc[rng, \"context_snippet\"], df.loc[rng, \"unit\"])\n",
        "]\n",
        "\n",
        "# basic dedupe: keep max-confidence per key\n",
        "keys = [\"page\",\"block_id\",\"category\",\"attribute\",\"value\",\"value_min\",\"value_max\",\"unit\",\"error\",\"error_unit\"]\n",
        "df = (df\n",
        "      .sort_values(\"confidence\", ascending=False)\n",
        "      .drop_duplicates(subset=keys, keep=\"first\")\n",
        "      .reset_index(drop=True))\n",
        "\n",
        "df.to_csv(\"/content/extracted_step4_clean.csv\", index=False)\n",
        "print(\"Saved cleaned CSV → /content/extracted_step4_clean.csv\")\n",
        "print(df.head(10).to_string(index=False))\n"
      ],
      "metadata": {
        "id": "3aMh1Rvlr4MZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab24b6a-91ec-411a-aea3-abf4726c9adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cleaned CSV → /content/extracted_step4_clean.csv\n",
            "Empty DataFrame\n",
            "Columns: [category, entity, attribute, value, unit, value_min, value_max, error, error_unit, method, mineral_phase, sample_id, context_snippet, page, block_id, confidence]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# step 5\n"
      ],
      "metadata": {
        "id": "S3VraUGpudN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, json\n",
        "import pandas as pd\n",
        "\n",
        "#A) Load input (change if needed)\n",
        "PREFERRED = \"/content/extracted_step4_clean.csv\"\n",
        "FALLBACKS = [\"/content/extracted_step4.csv\", \"/content/extracted_step4_offline.csv\"]\n",
        "\n",
        "def load_step4(path=PREFERRED, fallbacks=FALLBACKS):\n",
        "    if os.path.exists(path):\n",
        "        return pd.read_csv(path)\n",
        "    for fb in fallbacks:\n",
        "        if os.path.exists(fb):\n",
        "            print(f\"[Note] Using fallback: {fb}\")\n",
        "            return pd.read_csv(fb)\n",
        "    raise FileNotFoundError(\"No Step-4 CSV found. Expected one of: \"\n",
        "                            f\"{[path]+fallbacks}\")\n",
        "\n",
        "df = load_step4()\n",
        "print(f\"Loaded Step-4 facts: {len(df)} rows, columns={list(df.columns)}\")\n",
        "\n",
        "\n",
        "# Coerce numerics; normalize unit strings a bit (optional tweak)\n",
        "for col in [\"value\",\"value_min\",\"value_max\",\"error\",\"confidence\",\"page\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "def norm_unit(u):\n",
        "    if pd.isna(u): return None\n",
        "    s = str(u).strip().replace(\"ug/g\",\"µg/g\")\n",
        "    mapc = {\"ma\":\"Ma\",\"ga\":\"Ga\",\"°c\":\"°C\"}\n",
        "    return mapc.get(s.lower(), s)\n",
        "\n",
        "if \"unit\" in df.columns:\n",
        "    df[\"unit\"] = df[\"unit\"].apply(norm_unit)\n",
        "if \"error_unit\" in df.columns:\n",
        "    df[\"error_unit\"] = df[\"error_unit\"].apply(norm_unit)\n",
        "\n",
        "# Ensure essential columns exist\n",
        "for col in [\"category\",\"attribute\",\"context_snippet\",\"page\",\"block_id\",\"confidence\"]:\n",
        "    if col not in df.columns:\n",
        "        df[col] = None\n",
        "\n",
        "# Summary pivot\n",
        "summary_by_cat = df[\"category\"].value_counts().sort_values(ascending=False)\n",
        "summary_by_page = df.groupby(\"page\")[\"category\"].count().sort_index()\n",
        "\n",
        "print(\"\\n=== Step 5: Summary ===\")\n",
        "print(\"By category:\\n\", summary_by_cat.to_string())\n",
        "print(\"\\nBy page (non-zero only):\\n\", summary_by_page[summary_by_page>0].head(20).to_string())\n",
        "\n",
        "\n",
        "OUT_DIR = \"/content/step5_categories\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "cat_paths = {}\n",
        "for cat, sub in df.groupby(df[\"category\"].fillna(\"uncategorized\")):\n",
        "    path = os.path.join(OUT_DIR, f\"{cat}.csv\")\n",
        "    sub.sort_values([\"page\",\"block_id\",\"attribute\",\"value\"], inplace=True, kind=\"stable\")\n",
        "    sub.to_csv(path, index=False)\n",
        "    cat_paths[cat] = path\n",
        "\n",
        "print(f\"\\nSaved per-category CSVs → {OUT_DIR}\")\n",
        "for k, v in list(cat_paths.items())[:8]:\n",
        "    print(f\"  {k:16s} → {v}\")\n",
        "\n",
        "# Lookup tables\n",
        "# 1) Ages (geochronology)\n",
        "ages = df[df[\"category\"]==\"geochronology\"].copy()\n",
        "# prefer scalar 'value' else range min/max\n",
        "ages[\"age_value\"] = ages[\"value\"]\n",
        "ages[\"age_min\"]   = ages[\"value_min\"]\n",
        "ages[\"age_max\"]   = ages[\"value_max\"]\n",
        "ages_lookup = (ages[[\n",
        "    \"page\",\"block_id\",\"attribute\",\"age_value\",\"age_min\",\"age_max\",\n",
        "    \"error\",\"unit\",\"error_unit\",\"method\",\"mineral_phase\",\"sample_id\",\n",
        "    \"context_snippet\",\"confidence\"\n",
        "]]\n",
        "    .sort_values([\"page\",\"block_id\",\"age_value\",\"age_min\",\"age_max\",\"confidence\"], ascending=[True,True,True,True,True,False])\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "AGES_PATH = \"/content/geochronology_ages.csv\"\n",
        "ages_lookup.to_csv(AGES_PATH, index=False)\n",
        "\n",
        "assay = df[df[\"category\"]==\"assay\"].copy()\n",
        "assay_lookup = (assay[[\n",
        "    \"page\",\"block_id\",\"attribute\",\"value\",\"value_min\",\"value_max\",\"unit\",\n",
        "    \"method\",\"mineral_phase\",\"sample_id\",\"context_snippet\",\"confidence\"\n",
        "]]\n",
        "    .sort_values([\"attribute\",\"page\",\"block_id\",\"value\",\"value_min\",\"value_max\",\"confidence\"], ascending=[True,True,True,True,True,True,False])\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "ASSAY_PATH = \"/content/assay_grades.csv\"\n",
        "assay_lookup.to_csv(ASSAY_PATH, index=False)\n",
        "\n",
        "print(f\"\\nSaved lookups:\\n  Ages → {AGES_PATH}\\n  Assays → {ASSAY_PATH}\")\n",
        "\n",
        "\n",
        "CATALOG_PATH = \"/content/step5_catalog.jsonl\"\n",
        "with open(CATALOG_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, r in df.iterrows():\n",
        "\n",
        "        rec = {\n",
        "            \"category\": r.get(\"category\"),\n",
        "            \"attribute\": r.get(\"attribute\"),\n",
        "            \"unit\": r.get(\"unit\"),\n",
        "            \"value\": r.get(\"value\"),\n",
        "            \"value_min\": r.get(\"value_min\"),\n",
        "            \"value_max\": r.get(\"value_max\"),\n",
        "            \"error\": r.get(\"error\"),\n",
        "            \"page\": int(r.get(\"page\")) if pd.notna(r.get(\"page\")) else None,\n",
        "            \"block_id\": r.get(\"block_id\"),\n",
        "            \"confidence\": r.get(\"confidence\"),\n",
        "            \"context_snippet\": r.get(\"context_snippet\"),\n",
        "        }\n",
        "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "print(f\"Saved catalog JSONL → {CATALOG_PATH}\")\n",
        "\n",
        "\n",
        "def find_facts(query: str, category: str=None, attribute: str=None, top: int=30):\n",
        "    \"\"\"Keyword search across context + attribute with optional filters.\"\"\"\n",
        "    q = str(query).strip().lower()\n",
        "    sub = df.copy()\n",
        "    if category:\n",
        "        sub = sub[sub[\"category\"].str.lower()==category.lower()]\n",
        "    if attribute:\n",
        "        sub = sub[sub[\"attribute\"].fillna(\"\").str.lower()==attribute.lower()]\n",
        "    mask = (\n",
        "        sub[\"context_snippet\"].fillna(\"\").str.lower().str.contains(q, na=False) |\n",
        "        sub[\"attribute\"].fillna(\"\").str.lower().str.contains(q, na=False)\n",
        "    )\n",
        "    out = (sub[mask]\n",
        "           .sort_values([\"confidence\",\"page\"], ascending=[False,True])\n",
        "           .head(top)\n",
        "           .reset_index(drop=True))\n",
        "    return out[[\n",
        "        \"category\",\"attribute\",\"value\",\"value_min\",\"value_max\",\"unit\",\n",
        "        \"error\",\"error_unit\",\"page\",\"block_id\",\"confidence\",\"context_snippet\"\n",
        "    ]]\n",
        "\n",
        "\n",
        "print(\"\\nSample search: 'Au' (assay)\")\n",
        "print(find_facts(\"Au\", category=\"assay\").head(8).to_string(index=False))\n",
        "\n",
        "print(\"\\nSample search: '±' (ages with errors)\")\n",
        "print(find_facts(\"±\", category=\"geochronology\").head(8).to_string(index=False))\n",
        "\n",
        "# Final pointers\n",
        "print(\"\\nArtifacts:\")\n",
        "print(f\"  Per-category dir: {OUT_DIR}\")\n",
        "print(f\"  Ages lookup:      {AGES_PATH}\")\n",
        "print(f\"  Assay lookup:     {ASSAY_PATH}\")\n",
        "print(f\"  Catalog JSONL:    {CATALOG_PATH}\")\n"
      ],
      "metadata": {
        "id": "2Wv_LDdavbRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998380cb-6877-47b3-e283-766931190c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Step-4 facts: 0 rows, columns=['category', 'entity', 'attribute', 'value', 'unit', 'value_min', 'value_max', 'error', 'error_unit', 'method', 'mineral_phase', 'sample_id', 'context_snippet', 'page', 'block_id', 'confidence']\n",
            "\n",
            "=== Step 5: Summary ===\n",
            "By category:\n",
            " Series([], )\n",
            "\n",
            "By page (non-zero only):\n",
            " Series([], )\n",
            "\n",
            "Saved per-category CSVs → /content/step5_categories\n",
            "\n",
            "Saved lookups:\n",
            "  Ages → /content/geochronology_ages.csv\n",
            "  Assays → /content/assay_grades.csv\n",
            "Saved catalog JSONL → /content/step5_catalog.jsonl\n",
            "\n",
            "Sample search: 'Au' (assay)\n",
            "Empty DataFrame\n",
            "Columns: [category, attribute, value, value_min, value_max, unit, error, error_unit, page, block_id, confidence, context_snippet]\n",
            "Index: []\n",
            "\n",
            "Sample search: '±' (ages with errors)\n",
            "Empty DataFrame\n",
            "Columns: [category, attribute, value, value_min, value_max, unit, error, error_unit, page, block_id, confidence, context_snippet]\n",
            "Index: []\n",
            "\n",
            "Artifacts:\n",
            "  Per-category dir: /content/step5_categories\n",
            "  Ages lookup:      /content/geochronology_ages.csv\n",
            "  Assay lookup:     /content/assay_grades.csv\n",
            "  Catalog JSONL:    /content/step5_catalog.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 8) Referencing & Provenance Attachment\n",
        "!pip -q install pypdf pandas\n",
        "\n",
        "import re, json, os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from pypdf import PdfReader\n",
        "\n",
        "\n",
        "FACTS_CSV = \"/content/extracted_step4_clean.csv\"\n",
        "PDF_PATH  = \"/content/PhDThesis__Masurel2015.pdf\"  # same thesis used in Step 1\n",
        "\n",
        "assert os.path.exists(FACTS_CSV), f\"Missing {FACTS_CSV}\"\n",
        "assert os.path.exists(PDF_PATH),  f\"Missing {PDF_PATH}\"\n",
        "\n",
        "df = pd.read_csv(FACTS_CSV)\n",
        "\n",
        "\n",
        "def read_pdf_pages_text(pdf_path: str):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    out = []\n",
        "    for i, page in enumerate(reader.pages):\n",
        "        try:\n",
        "            txt = page.extract_text() or \"\"\n",
        "        except Exception:\n",
        "            txt = \"\"\n",
        "        out.append(txt)\n",
        "    return out\n",
        "\n",
        "pages_text = read_pdf_pages_text(PDF_PATH)\n",
        "N = len(pages_text)\n",
        "\n",
        "\n",
        "TITLE_PATS = [\n",
        "    r'^\\s*(chapter\\s+[ivx\\d]+\\.?\\s+.*)$',     # Chapter I / Chapter 1 ...\n",
        "    r'^\\s*chapter\\s+.*$',                     # \"Chapter ...\" loose\n",
        "    r'^\\s*\\d+\\.\\s+[A-Z].{3,}$',               # \"1. Title\"\n",
        "    r'^\\s*[A-Z][A-Za-z0-9\\-\\s]{4,}$',         # ALL/Title-like single line\n",
        "]\n",
        "TITLE_REGEX = re.compile(\"|\".join(TITLE_PATS), re.IGNORECASE | re.MULTILINE)\n",
        "\n",
        "def guess_section_title(page_text: str) -> str|None:\n",
        "    head = page_text.strip().splitlines()\n",
        "\n",
        "    for line in head[:25]:\n",
        "        m = TITLE_REGEX.search(line.strip())\n",
        "        if m:\n",
        "\n",
        "            if m.lastindex and m.group(m.lastindex):\n",
        "                return m.group(m.lastindex).strip()\n",
        "            return line.strip()\n",
        "    return None\n",
        "\n",
        "\n",
        "CITE_RE = re.compile(\n",
        "    r'\\(([A-Z][A-Za-z\\-]+(?:\\s*&\\s*[A-Z][A-Za-z\\-]+)?|[A-Z][A-Za-z\\-]+ et al\\.)\\s*,\\s*(\\d{4}[a-z]?)\\)',\n",
        "    re.UNICODE\n",
        ")\n",
        "\n",
        "def extract_citations(text: str, limit=6):\n",
        "    cits = [f\"{a} {y}\" for a, y in CITE_RE.findall(text)]\n",
        "    # de-dup, preserve order\n",
        "    seen, uniq = set(), []\n",
        "    for c in cits:\n",
        "        if c not in seen:\n",
        "            uniq.append(c); seen.add(c)\n",
        "    return uniq[:limit]\n",
        "\n",
        "\n",
        "provenance_rows = []\n",
        "for i, r in df.iterrows():\n",
        "    page = int(r.get(\"page\", 0))\n",
        "    pg_idx = max(0, min(N-1, page-1))\n",
        "\n",
        "    page_text = pages_text[pg_idx] if 0 <= pg_idx < N else \"\"\n",
        "    section_title = guess_section_title(page_text) or \"\"\n",
        "\n",
        "\n",
        "    context = str(r.get(\"context_snippet\") or \"\")\n",
        "    cites_ctx = extract_citations(context)\n",
        "    cites_pg  = extract_citations(page_text)\n",
        "    local_citations = list(dict.fromkeys(cites_ctx + cites_pg))\n",
        "\n",
        "    provenance_id = f\"{page}:{r.get('block_id')}\"\n",
        "    pdf_page_hint = f\"#page={page}\"\n",
        "    provenance_note = f\"thesis page {page}, block {r.get('block_id')}\"\n",
        "\n",
        "    provenance_rows.append({\n",
        "        \"provenance_id\": provenance_id,\n",
        "        \"pdf_page_hint\": pdf_page_hint,\n",
        "        \"section_title\": section_title,\n",
        "        \"local_citations\": \"; \".join(local_citations) if local_citations else \"\",\n",
        "        \"provenance_note\": provenance_note\n",
        "    })\n",
        "\n",
        "prov = pd.DataFrame(provenance_rows, index=df.index)\n",
        "\n",
        "# Merge & save\n",
        "df_enriched = pd.concat([df, prov], axis=1)\n",
        "\n",
        "OUT_ENRICHED = \"/content/extracted_step8_provenance.csv\"\n",
        "df_enriched.to_csv(OUT_ENRICHED, index=False)\n",
        "print(f\"Saved with provenance → {OUT_ENRICHED}\")\n",
        "\n",
        "\n",
        "show_cols = [\"category\",\"attribute\",\"value\",\"value_min\",\"value_max\",\"unit\",\"error\",\"page\",\"block_id\",\"section_title\",\"local_citations\",\"provenance_id\"]\n",
        "print(df_enriched[show_cols].head(10).to_string(index=False))\n"
      ],
      "metadata": {
        "id": "4N8UqJf4yyE-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "fa933604-a0c3-4ac2-eabe-a68bec3c2e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved with provenance → /content/extracted_step8_provenance.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['section_title', 'local_citations', 'provenance_id'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2811422592.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m# --- Quick peek ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0mshow_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"category\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"attribute\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"value_min\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"value_max\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"unit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"page\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"block_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"section_title\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"local_citations\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"provenance_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_enriched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshow_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['section_title', 'local_citations', 'provenance_id'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 9 output writers ( master table + category sheets)"
      ],
      "metadata": {
        "id": "cpWFljJ33Ekf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, re, json, uuid, hashlib\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "FACTS_IN   = \"/content/extracted_step8_provenance.csv\"   # from Step 8\n",
        "PDF_PATH   = \"/content/PhDThesis__Masurel2015.pdf\"       # same as Step 1\n",
        "THESIS_ID  = \"Masurel2015\"\n",
        "PIPE_VER   = \"v0.9-step9\"\n",
        "\n",
        "assert os.path.exists(FACTS_IN), f\"Missing {FACTS_IN}\"\n",
        "\n",
        "df = pd.read_csv(FACTS_IN)\n",
        "print(f\"Loaded Step-8 enriched facts: {len(df)} rows\")\n",
        "\n",
        "\n",
        "ALLOWED_CATEGORIES = {\n",
        "    \"geochronology\",\"assay\",\"geochemistry\",\"lithology\",\"minerals\",\n",
        "    \"mineralisation\",\"structures\",\"tectonism_event\",\"methods\",\"stratigraphy\",\"rock_type\"\n",
        "}\n",
        "UNIT_MAP_LOWER = {\"ma\":\"Ma\",\"ga\":\"Ga\",\"°c\":\"°C\",\"ug/g\":\"µg/g\",\"percent\":\"%\",\"gpt\":\"g/t\"}\n",
        "def norm_unit(u):\n",
        "    if pd.isna(u): return None\n",
        "    s = str(u).strip()\n",
        "    s2 = UNIT_MAP_LOWER.get(s.lower(), s)\n",
        "    return s2\n",
        "\n",
        "def norm_category(c):\n",
        "    if pd.isna(c): return \"assay\"\n",
        "    s = str(c).strip().lower().replace(\"–\",\"-\")\n",
        "    # canonical map\n",
        "    m = {\n",
        "        \"geochronology\":\"geochronology\",\"assay\":\"assay\",\"geochemistry\":\"geochemistry\",\n",
        "        \"lithology\":\"lithology\",\"minerals\":\"minerals\",\"mineralisation\":\"mineralisation\",\n",
        "        \"mineralization\":\"mineralisation\",\"structures\":\"structures\",\n",
        "        \"tectonism_event\":\"tectonism_event\",\"methods\":\"methods\",\n",
        "        \"stratigraphy\":\"stratigraphy\",\"rock_type\":\"rock_type\"\n",
        "    }\n",
        "    return m.get(s, \"assay\")\n",
        "\n",
        "for c in [\"value\",\"value_min\",\"value_max\",\"error\",\"confidence\",\"page\"]:\n",
        "    if c in df.columns:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "if \"unit\" in df.columns:       df[\"unit\"] = df[\"unit\"].apply(norm_unit)\n",
        "if \"error_unit\" in df.columns: df[\"error_unit\"] = df[\"error_unit\"].apply(norm_unit)\n",
        "if \"category\" in df.columns:   df[\"category\"] = df[\"category\"].apply(norm_category)\n",
        "\n",
        "\n",
        "def make_stable_id(row):\n",
        "    key = \"|\".join([\n",
        "        str(row.get(\"category\", \"\")),\n",
        "        str(row.get(\"attribute\",\"\")),\n",
        "        str(row.get(\"value\",\"\")),\n",
        "        str(row.get(\"value_min\",\"\")),\n",
        "        str(row.get(\"value_max\",\"\")),\n",
        "        str(row.get(\"unit\",\"\")),\n",
        "        str(row.get(\"error\",\"\")),\n",
        "        str(row.get(\"page\",\"\")),\n",
        "        str(row.get(\"block_id\",\"\")),\n",
        "    ])\n",
        "    return str(uuid.uuid5(uuid.NAMESPACE_URL, key))\n",
        "\n",
        "df[\"fact_id\"]    = df.apply(make_stable_id, axis=1)\n",
        "df[\"thesis_id\"]  = THESIS_ID\n",
        "df[\"pdf_path\"]   = PDF_PATH if os.path.exists(PDF_PATH) else \"\"\n",
        "df[\"pipeline_version\"] = PIPE_VER\n",
        "\n",
        "\n",
        "def quality_flag(row):\n",
        "    has_scalar = pd.notna(row.get(\"value\"))\n",
        "    has_range  = pd.notna(row.get(\"value_min\")) or pd.notna(row.get(\"value_max\"))\n",
        "    has_num    = bool(has_scalar or has_range or pd.notna(row.get(\"error\")))\n",
        "    unit       = row.get(\"unit\")\n",
        "    cat        = row.get(\"category\",\"\")\n",
        "    method     = str(row.get(\"method\") or \"\").strip()\n",
        "    prov_ok    = str(row.get(\"provenance_id\") or \"\")!=\"\"\n",
        "    # Rules (worst-first)\n",
        "    if not has_num:\n",
        "        return \"warn_incomplete\"\n",
        "    if cat in (\"assay\",\"geochemistry\") and not unit:\n",
        "        return \"warn_missing_unit\"\n",
        "    if cat==\"geochronology\":\n",
        "        # ages should carry unit Ma/Ga\n",
        "        if not unit:\n",
        "            return \"warn_missing_unit\"\n",
        "        if method==\"\":\n",
        "            return \"warn_missing_method\"\n",
        "    if not prov_ok:\n",
        "        return \"warn_missing_provenance\"\n",
        "    return \"ok\"\n",
        "\n",
        "df[\"quality_flag\"] = df.apply(quality_flag, axis=1)\n",
        "\n",
        "\n",
        "MASTER_COLS = [\n",
        "    \"fact_id\",\"thesis_id\",\"pdf_path\",\"pipeline_version\",\n",
        "    \"category\",\"entity\",\"attribute\",\n",
        "    \"value\",\"value_min\",\"value_max\",\"unit\",\"error\",\"error_unit\",\n",
        "    \"method\",\"mineral_phase\",\"sample_id\",\n",
        "    \"page\",\"block_id\",\"section_title\",\"local_citations\",\"context_snippet\",\n",
        "    \"provenance_id\",\"quality_flag\",\"confidence\"\n",
        "]\n",
        "for col in MASTER_COLS:\n",
        "    if col not in df.columns:\n",
        "        df[col] = None\n",
        "master = df[MASTER_COLS].copy()\n",
        "\n",
        "\n",
        "OUT_DIR = \"/content/step9_outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "MASTER_CSV  = f\"{OUT_DIR}/master_facts.csv\"\n",
        "MASTER_PARQ = f\"{OUT_DIR}/master_facts.parquet\"\n",
        "master.to_csv(MASTER_CSV, index=False)\n",
        "try:\n",
        "    master.to_parquet(MASTER_PARQ, index=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(f\"Saved Master CSV   → {MASTER_CSV}\")\n",
        "print(f\"Saved Master PARQ  → {MASTER_PARQ}\")\n",
        "\n",
        "\n",
        "!pip install -q xlsxwriter\n",
        "XLSX_PATH = f\"{OUT_DIR}/Geo_Master_and_Categories.xlsx\"\n",
        "with pd.ExcelWriter(XLSX_PATH, engine=\"xlsxwriter\") as xw:\n",
        "    ...\n",
        "\n",
        "\n",
        "XLSX_PATH = f\"{OUT_DIR}/Geo_Master_and_Categories.xlsx\"\n",
        "with pd.ExcelWriter(XLSX_PATH, engine=\"xlsxwriter\") as xw:\n",
        "    # Overview\n",
        "    ov = (master[\"category\"].value_counts()\n",
        "            .rename_axis(\"category\")\n",
        "            .reset_index(name=\"count\")\n",
        "            .sort_values(\"count\", ascending=False))\n",
        "    ov[\"percent\"] = (ov[\"count\"]/len(master)*100).round(1)\n",
        "    qf = (master[\"quality_flag\"].value_counts()\n",
        "            .rename_axis(\"quality_flag\")\n",
        "            .reset_index(name=\"count\")\n",
        "            .sort_values(\"count\", ascending=False))\n",
        "    ov.to_excel(xw, sheet_name=\"Overview\", index=False)\n",
        "    qf.to_excel(xw, sheet_name=\"Quality\",  index=False)\n",
        "\n",
        "    # Per-category sheets\n",
        "    order = [\"geochronology\",\"assay\",\"geochemistry\",\"lithology\",\"minerals\",\n",
        "             \"mineralisation\",\"structures\",\"tectonism_event\",\"methods\",\"stratigraphy\",\"rock_type\"]\n",
        "    cats = sorted(master[\"category\"].dropna().unique(), key=lambda c: (order.index(c) if c in order else 999, c))\n",
        "    for cat in cats:\n",
        "        sub = master[master[\"category\"]==cat].copy()\n",
        "        sub = sub.sort_values([\"page\",\"block_id\",\"attribute\",\"value\",\"value_min\",\"value_max\",\"confidence\"],\n",
        "                              ascending=[True,True,True,True,True,True,False])\n",
        "        # keep most useful columns for browsing\n",
        "        view_cols = [\n",
        "            \"fact_id\",\"attribute\",\"value\",\"value_min\",\"value_max\",\"unit\",\"error\",\"error_unit\",\n",
        "            \"method\",\"mineral_phase\",\"sample_id\",\"page\",\"block_id\",\"section_title\",\n",
        "            \"local_citations\",\"quality_flag\",\"confidence\",\"context_snippet\"\n",
        "        ]\n",
        "        sub[view_cols].to_excel(xw, sheet_name=cat[:31], index=False)\n",
        "\n",
        "print(f\"Saved Excel (master + sheets) → {XLSX_PATH}\")\n",
        "\n",
        "\n",
        "print(\"\\n=== Step 9 Summary ===\")\n",
        "print(\"By category:\\n\", master[\"category\"].value_counts().to_string())\n",
        "print(\"\\nBy quality_flag:\\n\", master[\"quality_flag\"].value_counts().to_string())\n",
        "\n",
        "bad = master[master[\"quality_flag\"]!=\"ok\"].head(12)\n",
        "if len(bad):\n",
        "    print(\"\\nSample non-OK rows:\\n\", bad[[\"category\",\"attribute\",\"value\",\"unit\",\"method\",\"page\",\"block_id\",\"quality_flag\"]].to_string(index=False))\n"
      ],
      "metadata": {
        "id": "IY3T7qx53LQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9837d2de-fd1a-4d03-dbc5-9f7733978eb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Step-8 enriched facts: 0 rows\n",
            "Saved Master CSV   → /content/step9_outputs/master_facts.csv\n",
            "Saved Master PARQ  → /content/step9_outputs/master_facts.parquet\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSaved Excel (master + sheets) → /content/step9_outputs/Geo_Master_and_Categories.xlsx\n",
            "\n",
            "=== Step 9 Summary ===\n",
            "By category:\n",
            " Series([], )\n",
            "\n",
            "By quality_flag:\n",
            " Series([], )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, re, uuid\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "MASTER_IN   = \"/content/step9_outputs/master_facts.csv\"     # from Step 9\n",
        "PDF_PATH    = \"/content/PhDThesis__Masurel2015.pdf\"         # optional (helps page-level unit inference)\n",
        "OUT_DIR     = \"/content/step9_outputs\"\n",
        "XLSX_PATH   = f\"{OUT_DIR}/Geo_Master_and_Categories.xlsx\"\n",
        "\n",
        "assert os.path.exists(MASTER_IN), f\"Missing {MASTER_IN}\"\n",
        "df = pd.read_csv(MASTER_IN)\n",
        "\n",
        "\n",
        "pages_text = None\n",
        "if os.path.exists(PDF_PATH):\n",
        "    try:\n",
        "        from pypdf import PdfReader\n",
        "        pages_text = []\n",
        "        rdr = PdfReader(PDF_PATH)\n",
        "        for p in rdr.pages:\n",
        "            try:\n",
        "                pages_text.append(p.extract_text() or \"\")\n",
        "            except Exception:\n",
        "                pages_text.append(\"\")\n",
        "        print(f\"[info] Loaded {len(pages_text)} pages for backfill context.\")\n",
        "    except Exception as e:\n",
        "        print(\"[warn] Could not load PDF for page text:\", e)\n",
        "\n",
        "def page_text(page_1based:int) -> str:\n",
        "    if pages_text is None: return \"\"\n",
        "    i = max(0, min(len(pages_text)-1, int(page_1based)-1))\n",
        "    return pages_text[i]\n",
        "\n",
        "\n",
        "UNIT_CUES = {\n",
        "    \"Ma\": re.compile(r'\\bMa\\b', re.I),\n",
        "    \"Ga\": re.compile(r'\\bGa\\b', re.I),\n",
        "    \"ppm\": re.compile(r'\\bppm\\b', re.I),\n",
        "    \"ppb\": re.compile(r'\\bppb\\b', re.I),\n",
        "    \"g/t\": re.compile(r'\\bg\\/t\\b', re.I),\n",
        "    \"%\":   re.compile(r'(?<!wt)\\b%\\b', re.I),\n",
        "    \"wt%\": re.compile(r'\\bwt%\\b', re.I),\n",
        "}\n",
        "METHOD_CUES = {\n",
        "    \"U–Pb\":   re.compile(r'\\bU[–-]Pb\\b', re.I),\n",
        "    \"Pb–Pb\":  re.compile(r'\\bPb[–-]Pb\\b', re.I),\n",
        "    \"Sm–Nd\":  re.compile(r'\\bSm[–-]Nd\\b', re.I),\n",
        "    \"Ar–Ar\":  re.compile(r'\\bAr[–-]Ar\\b', re.I),\n",
        "    \"Rb–Sr\":  re.compile(r'\\bRb[–-]Sr\\b', re.I),\n",
        "    \"Lu–Hf\":  re.compile(r'\\bLu[–-]Hf\\b', re.I),\n",
        "    \"Re–Os\":  re.compile(r'\\bRe[–-]Os\\b', re.I),\n",
        "}\n",
        "PHASE_CUES = {\n",
        "    \"zircon\": re.compile(r'\\bzircon\\b', re.I),\n",
        "    \"monazite\": re.compile(r'\\bmonazite\\b', re.I),\n",
        "    \"baddeleyite\": re.compile(r'\\bbaddeleyite\\b', re.I),\n",
        "}\n",
        "\n",
        "def infer_from_text(text: str, cues: dict[str, re.Pattern]) -> list[str]:\n",
        "    hits = []\n",
        "    low = text or \"\"\n",
        "    for key, pat in cues.items():\n",
        "        if pat.search(low):\n",
        "            hits.append(key)\n",
        "    return hits\n",
        "\n",
        "\n",
        "before_missing_units = (df[\"unit\"].isna() | (df[\"unit\"].astype(str).str.strip()==\"\")).sum()\n",
        "\n",
        "for i, r in df.iterrows():\n",
        "    cat   = str(r.get(\"category\",\"\")).lower().strip()\n",
        "    unit  = (None if pd.isna(r.get(\"unit\")) else str(r.get(\"unit\")).strip()) or None\n",
        "    meth  = (None if pd.isna(r.get(\"method\")) else str(r.get(\"method\")).strip()) or None\n",
        "    phase = (None if pd.isna(r.get(\"mineral_phase\")) else str(r.get(\"mineral_phase\")).strip()) or None\n",
        "\n",
        "    # Build context (snippet + page)\n",
        "    ctx = str(r.get(\"context_snippet\") or \"\")\n",
        "    pg  = int(r.get(\"page\") or 0)\n",
        "    full = (ctx + \" \" + page_text(pg)).strip()\n",
        "\n",
        "    # 1) Unit backfill\n",
        "    if unit is None:\n",
        "        # geochronology first: look for Ma/Ga\n",
        "        if cat == \"geochronology\":\n",
        "            if UNIT_CUES[\"Ma\"].search(full):\n",
        "                unit = \"Ma\"\n",
        "            elif UNIT_CUES[\"Ga\"].search(full):\n",
        "                unit = \"Ga\"\n",
        "        # assays/geochem\n",
        "        if unit is None:\n",
        "            for key in [\"g/t\",\"ppm\",\"ppb\",\"wt%\",\"%\"]:\n",
        "                if UNIT_CUES[key].search(full):\n",
        "                    unit = key\n",
        "                    break\n",
        "        if unit is not None:\n",
        "            df.at[i, \"unit\"] = unit\n",
        "\n",
        "    # 2) Method backfill (if obvious)\n",
        "    if not meth:\n",
        "        hits = infer_from_text(full, METHOD_CUES)\n",
        "        if hits:\n",
        "            df.at[i, \"method\"] = hits[0]  # first strongest\n",
        "\n",
        "    # 3) Mineral phase backfill (if obvious)\n",
        "    if not phase and cat == \"geochronology\":\n",
        "        ph = infer_from_text(full, PHASE_CUES)\n",
        "        if ph:\n",
        "            df.at[i, \"mineral_phase\"] = ph[0]\n",
        "\n",
        "after_missing_units = (df[\"unit\"].isna() | (df[\"unit\"].astype(str).str.strip()==\"\")).sum()\n",
        "print(f\"Units filled: {before_missing_units - after_missing_units} rows\")\n",
        "\n",
        "\n",
        "def recompute_qf(row):\n",
        "    has_scalar = pd.notna(row.get(\"value\"))\n",
        "    has_range  = pd.notna(row.get(\"value_min\")) or pd.notna(row.get(\"value_max\"))\n",
        "    has_num    = bool(has_scalar or has_range or pd.notna(row.get(\"error\")))\n",
        "    unit       = (None if pd.isna(row.get(\"unit\")) else str(row.get(\"unit\")).strip()) or None\n",
        "    cat        = str(row.get(\"category\",\"\"))\n",
        "    method     = str(row.get(\"method\") or \"\").strip()\n",
        "    prov_ok    = str(row.get(\"provenance_id\") or \"\")!=\"\"\n",
        "    if not has_num:\n",
        "        return \"warn_incomplete\"\n",
        "    if cat in (\"assay\",\"geochemistry\") and not unit:\n",
        "        return \"warn_missing_unit\"\n",
        "    if cat==\"geochronology\":\n",
        "        if not unit:\n",
        "            return \"warn_missing_unit\"\n",
        "        if method==\"\":\n",
        "            return \"warn_missing_method\"\n",
        "    if not prov_ok:\n",
        "        return \"warn_missing_provenance\"\n",
        "    return \"ok\"\n",
        "\n",
        "df[\"quality_flag\"] = df.apply(recompute_qf, axis=1)\n",
        "\n",
        "\n",
        "MASTER_CSV  = f\"{OUT_DIR}/master_facts.csv\"\n",
        "MASTER_PARQ = f\"{OUT_DIR}/master_facts.parquet\"\n",
        "df.to_csv(MASTER_CSV, index=False)\n",
        "try:\n",
        "    df.to_parquet(MASTER_PARQ, index=False)\n",
        "except Exception:\n",
        "    pass\n",
        "print(\"Re-saved Master CSV/PARQ with backfills.\")\n",
        "\n",
        "# Excel rewrite\n",
        "!pip -q install xlsxwriter\n",
        "with pd.ExcelWriter(XLSX_PATH, engine=\"xlsxwriter\") as xw:\n",
        "    ov = (df[\"category\"].value_counts()\n",
        "          .rename_axis(\"category\")\n",
        "          .reset_index(name=\"count\")\n",
        "          .sort_values(\"count\", ascending=False))\n",
        "    ov[\"percent\"] = (ov[\"count\"]/len(df)*100).round(1)\n",
        "    qf = (df[\"quality_flag\"].value_counts()\n",
        "          .rename_axis(\"quality_flag\")\n",
        "          .reset_index(name=\"count\")\n",
        "          .sort_values(\"count\", ascending=False))\n",
        "    ov.to_excel(xw, sheet_name=\"Overview\", index=False)\n",
        "    qf.to_excel(xw, sheet_name=\"Quality\",  index=False)\n",
        "\n",
        "    order = [\"geochronology\",\"assay\",\"geochemistry\",\"lithology\",\"minerals\",\n",
        "             \"mineralisation\",\"structures\",\"tectonism_event\",\"methods\",\"stratigraphy\",\"rock_type\"]\n",
        "    cats = sorted(df[\"category\"].dropna().unique(), key=lambda c: (order.index(c) if c in order else 999, c))\n",
        "    for cat in cats:\n",
        "        sub = df[df[\"category\"]==cat].copy()\n",
        "        sub = sub.sort_values([\"page\",\"block_id\",\"attribute\",\"value\",\"value_min\",\"value_max\",\"confidence\"],\n",
        "                              ascending=[True,True,True,True,True,True,False])\n",
        "        cols = [\n",
        "            \"fact_id\",\"attribute\",\"value\",\"value_min\",\"value_max\",\"unit\",\"error\",\"error_unit\",\n",
        "            \"method\",\"mineral_phase\",\"sample_id\",\"page\",\"block_id\",\"section_title\",\n",
        "            \"local_citations\",\"quality_flag\",\"confidence\",\"context_snippet\"\n",
        "        ]\n",
        "        cols = [c for c in cols if c in sub.columns]\n",
        "        sub[cols].to_excel(xw, sheet_name=cat[:31], index=False)\n",
        "\n",
        "print(\"Updated Excel written.\")\n"
      ],
      "metadata": {
        "id": "gww5kb_25tT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vfy6NkEy8zI_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AznhvS2V80Yp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}