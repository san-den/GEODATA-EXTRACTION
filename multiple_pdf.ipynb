{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pdfplumber nltk pandas numpy openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUmhvAImYhT0",
        "outputId": "eb8a1fa5-1e7b-4e0b-bcbc-c152be46e522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup, API key, and PDF ingestion\n",
        "**Purpose**: Install dependencies, load OpenAI key from Colab Secret `sandra`, and move any uploaded PDFs into `/content/pdfs`.\n",
        "\n",
        "**Inputs**: PDFs in `/content/` (uploaded via Colab), Secret `sandra`.\n",
        "\n",
        "**Outputs**: `client = OpenAI(...)` ready; PDFs consolidated under `/content/pdfs`."
      ],
      "metadata": {
        "id": "kUYYNzhREMaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install openai pdfplumber pandas\n",
        "\n",
        "from pathlib import Path\n",
        "import pdfplumber, re, json, os\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# 🔑 Load your OpenAI key correctly (secret name = \"sandra\")\n",
        "api_key = userdata.get(\"sandra\")\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "PDF_DIR = Path(\"/content/pdfs\")\n",
        "PDF_DIR.mkdir(exist_ok=True, parents=True)\n",
        "for p in Path(\"/content\").glob(\"*.pdf\"):\n",
        "    shutil.move(str(p), str(PDF_DIR / p.name))\n",
        "\n",
        "print(\"Now in /content/pdfs:\", [p.name for p in PDF_DIR.glob(\"*.pdf\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X32-zP9-YkRA",
        "outputId": "ae7a418d-a1f2-4165-f774-00d2161350c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now in /content/pdfs: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage 2: Page-Aware Text Extraction\n",
        "**Purpose**: Extract clean text *per page* (no global join/split), lightly normalize whitespace, de-hyphenate lines, remove an initial **Abstract** only on the first pages, and stop once a page **starts** with a References/Bibliography heading.\n",
        "\n",
        "**Input**: `pdf_path: Path`.\n",
        "\n",
        "**Output**: `[(page_num:int, text:str), ...]` up to (but not including) the first references page."
      ],
      "metadata": {
        "id": "-N0KWvK1EQNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Stage 2: Page-Aware Text Extraction (safer, no re-split) ===\n",
        "import re, pdfplumber\n",
        "from pathlib import Path\n",
        "\n",
        "def extract_clean_text_pages(pdf_path: Path):\n",
        "    \"\"\"\n",
        "    Return: list[(page_num:int, text:str)]\n",
        "    - Cleans hyphenated line breaks & excess newlines\n",
        "    - Drops Abstract only on the first few pages\n",
        "    - Stops at the first page that *starts with* a References/Bibliography heading\n",
        "    - No global join/re-split (so no content loss)\n",
        "    \"\"\"\n",
        "    pages = []\n",
        "    with pdfplumber.open(str(pdf_path)) as pdf:\n",
        "        for i, page in enumerate(pdf.pages, start=1):\n",
        "            t = page.extract_text() or \"\"\n",
        "            # de-hyphenate across line breaks\n",
        "            t = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", t)\n",
        "            # normalize whitespace\n",
        "            t = re.sub(r\"[ \\t]+\", \" \", t)\n",
        "            t = re.sub(r\"\\n{2,}\", \"\\n\", t).strip()\n",
        "            pages.append((i, t))\n",
        "\n",
        "    # Remove Abstract only within the first 3 pages (if present)\n",
        "    for k in range(min(3, len(pages))):\n",
        "        pnum, txt = pages[k]\n",
        "        # remove a leading Abstract block conservatively\n",
        "        txt2 = re.sub(\n",
        "            r\"(?is)^\\s*abstract\\b.*?(?=\\n[A-Z][^\\n]{0,80}\\n|\\Z)\",  # up to next likely heading or end\n",
        "            \"\",\n",
        "            txt,\n",
        "            count=1\n",
        "        ).strip()\n",
        "        pages[k] = (pnum, txt2)\n",
        "\n",
        "    # Find the first page that LOOKS like a references page and stop there.\n",
        "    def is_refs_page(txt: str) -> bool:\n",
        "        # only count if heading near the top of the page\n",
        "        head = \"\\n\".join(txt.splitlines()[:20])\n",
        "        return bool(re.search(r\"(?im)^\\s*(references|bibliograph\\w*)\\s*$\", head))\n",
        "\n",
        "    stop_at = None\n",
        "    for idx, (pnum, txt) in enumerate(pages):\n",
        "        if is_refs_page(txt):\n",
        "            stop_at = idx\n",
        "            break\n",
        "    if stop_at is not None:\n",
        "        pages = pages[:stop_at]  # cut at the references page, keep original page splits\n",
        "\n",
        "    return pages"
      ],
      "metadata": {
        "id": "vOMFakj0Ysgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build page corpus for all PDFs\n",
        "**Purpose**: Run Stage 2 over `/content/pdfs/*.pdf` and store results in memory for the LLM stage.\n",
        "\n",
        "**Input**: PDFs in `/content/pdfs`.\n",
        "\n",
        "**Output**: `all_docs_pages = { \"<pdf_name>\": [(page_num, text), ...], ... }`."
      ],
      "metadata": {
        "id": "RezQ5JT5Ed3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect pages for each PDF (no merging)\n",
        "PDF_DIR = Path(\"/content/pdfs\")\n",
        "pdf_paths = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
        "assert pdf_paths, f\"No PDFs found in {PDF_DIR}. Upload PDFs first.\"\n",
        "\n",
        "all_docs_pages = {}  # {pdf_name: [(page_num, text), ...]}\n",
        "for pdf_path in pdf_paths:\n",
        "    pages = extract_clean_text_pages(pdf_path)\n",
        "    all_docs_pages[pdf_path.name] = pages\n",
        "    print(f\"✅ {pdf_path.name}: {len(pages)} pages kept (pre-LLM)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4FjMp126X5-",
        "outputId": "0dfefd7c-2246-46cc-be0b-6c165457a18e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf: 78 pages kept (pre-LLM)\n",
            "✅ 2011_Peters_East Markoye_2011.pdf: 62 pages kept (pre-LLM)\n",
            "✅ 2015_Masurel_phd.pdf: 257 pages kept (pre-LLM)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage 3: Page-referenced LLM extraction (schema + ask_model)\n",
        "**Purpose**: Define the extraction **schema** and the `ask_model(page_num, page_text)` function. The function:\n",
        "- Sends only the **current page** text to `gpt-4o-mini`.\n",
        "- Forces **pure JSON** output.\n",
        "- Forbids guessing: missing fields must be `\"\"`, `null`, `0`, or `[]`.\n",
        "- Deduplicates lists.\n",
        "\n",
        "**Inputs**: `page_num`, `page_text`.\n",
        "\n",
        "**Output**: JSON object adhering to the schema (or `[]` if nothing relevant)."
      ],
      "metadata": {
        "id": "LcxdQ_AbExWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import json, textwrap\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "schema = {\n",
        "  \"page_number\": \"integer\",\n",
        "  \"metadata\": {\n",
        "    \"title\": \"string\",\n",
        "    \"author\": \"string\",\n",
        "    \"year\": \"integer\",\n",
        "    \"supervisor\": \"string\",\n",
        "    \"institution\": \"string\",\n",
        "    \"location\": \"string\"\n",
        "  },\n",
        "  \"geology\": {\n",
        "    \"region\": \"string\",\n",
        "    \"formation\": \"string\",\n",
        "    \"rock_types\": [\"list of strings\"],\n",
        "    \"minerals\": [\"list of strings\"],\n",
        "    \"structures\": [\"list of strings\"],\n",
        "    \"tectonic_setting\": \"string\"\n",
        "  },\n",
        "  \"geochronology\": {\n",
        "    \"sample_id\": \"string\",\n",
        "    \"method\": \"string\",\n",
        "    \"age_Ma\": \"float\",\n",
        "    \"error_Ma\": \"float\",\n",
        "    \"rock_unit\": \"string\",\n",
        "    \"evidence\": \"string\"\n",
        "  },\n",
        "  \"geochemistry\": {\n",
        "    \"sample_id\": \"string\",\n",
        "    \"analyte\": \"string\",\n",
        "    \"value\": \"float\",\n",
        "    \"unit\": \"string\",\n",
        "    \"method\": \"string\",\n",
        "    \"context\": \"string\"\n",
        "  },\n",
        "  \"metallogeny\": {\n",
        "    \"mineralisation_type\": \"string\",\n",
        "    \"associated_structures\": [\"list of strings\"],\n",
        "    \"host_rocks\": [\"list of strings\"],\n",
        "    \"ore_minerals\": [\"list of strings\"],\n",
        "    \"alteration\": \"string\"\n",
        "  }\n",
        "}\n",
        "\n",
        "def ask_model(page_num: int, page_text: str, desc=\"Extracting\"):\n",
        "    # keep per-page cap; most pages are << 5500 chars anyway\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\":\n",
        "         \"You are a geology data extraction AI. Output valid JSON ONLY.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"\n",
        "Extract ONLY from the page text given. Obey this schema exactly:\n",
        "{json.dumps(schema, indent=2)}\n",
        "\n",
        "Rules:\n",
        "- Set \"page_number\": {page_num}.\n",
        "- Use ONLY facts present in the page text. Do NOT infer, guess, or use outside knowledge.\n",
        "- If a field is not present, set empty string (\"\"), 0, null, or [] as appropriate (NO 'Unknown').\n",
        "- If nothing relevant is present, return [] (empty list).\n",
        "- Keep lists deduplicated.\n",
        "\n",
        "PAGE TEXT (page {page_num}):\n",
        "{text[:5500]}\n",
        "\"\"\"}\n",
        "    ]\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages,\n",
        "        temperature=0.1,\n",
        "        max_tokens=1200,\n",
        "        response_format={\"type\": \"json_object\"}  # force JSON\n",
        "    )\n",
        "    raw = resp.choices[0].message.content.strip()\n",
        "    try:\n",
        "        data = json.loads(raw)\n",
        "    except Exception:\n",
        "        # try to salvage JSON object from code fences if present\n",
        "        raw2 = raw.strip(\"`\").replace(\"json\", \"\").strip()\n",
        "        data = json.loads(raw2)\n",
        "    return data\n",
        "\n",
        "# Run per PDF → each gets its own extracted JSON\n",
        "OUT_DIR = Path(\"/content/extracted_json\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for pdf_name, page_list in all_docs_pages.items():\n",
        "    out_path = OUT_DIR / f\"{Path(pdf_name).stem}.extracted.json\"\n",
        "    records = []\n",
        "    for pnum, text in tqdm(page_list, desc=f\"Extracting {pdf_name}\", leave=False):\n",
        "        if not text:\n",
        "            continue\n",
        "        try:\n",
        "            rec = ask_model(pnum, text)\n",
        "            if rec:\n",
        "                records.append(rec)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ {pdf_name} p{pnum}: {e}\")\n",
        "\n",
        "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(records, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"✅ Saved {len(records)} page records → {out_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZE5eRj0xfk0",
        "outputId": "44049080-5848-47ab-a625-9685e691a601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 78 page records → /content/extracted_json/2007_Tshibubudze_THE MARKOYE FAULT_2007.extracted.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 62 page records → /content/extracted_json/2011_Peters_East Markoye_2011.extracted.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 257 page records → /content/extracted_json/2015_Masurel_phd.extracted.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Stage 3 per PDF and write outputs\n",
        "**Purpose**: Iterate `all_docs_pages`, call `ask_model()` for each page, and write one JSON file per PDF.\n",
        "\n",
        "**Input**: `all_docs_pages` dict.\n",
        "\n",
        "**Output**: `/content/extracted_json/<stem>.extracted.json` (list of page records)."
      ],
      "metadata": {
        "id": "54SYv-fnE6D5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "IN_DIR  = Path(\"/content/extracted_json\")       # outputs of Stage 3\n",
        "OUT_DIR = Path(\"/content/flattened_csv\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def normalize_records(raw):\n",
        "    \"\"\"Stage-3 may store a list of dicts, or a dict per page. Normalize to list[dict].\"\"\"\n",
        "    if not raw:\n",
        "        return []\n",
        "    out = []\n",
        "    for item in raw:\n",
        "        if isinstance(item, list):\n",
        "            out.extend([x for x in item if isinstance(x, dict)])\n",
        "        elif isinstance(item, dict):\n",
        "            out.append(item)\n",
        "    return out\n",
        "\n",
        "def flatten_one_pdf(pdf_name: str, recs: list[dict]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Flatten per-PDF:\n",
        "      - de-dup (section, field, value) within this PDF\n",
        "      - union the page numbers where it appears\n",
        "    \"\"\"\n",
        "    combined = defaultdict(lambda: {\n",
        "        \"pdf_name\": pdf_name, \"section\": None, \"field\": None, \"value\": None, \"pages\": set()\n",
        "    })\n",
        "\n",
        "    for rec in recs:\n",
        "        page = rec.get(\"page_number\", None)\n",
        "        for section, fields in rec.items():\n",
        "            if section == \"page_number\":\n",
        "                continue\n",
        "\n",
        "            # section: metadata, geology, geochronology, geochemistry, metallogeny\n",
        "            if isinstance(fields, dict):\n",
        "                for k, v in fields.items():\n",
        "                    # keep zeros and False; skip only None or empty strings\n",
        "                    if v is None or (isinstance(v, str) and v.strip() == \"\"):\n",
        "                        continue\n",
        "                    if isinstance(v, list):\n",
        "                        for vv in v:\n",
        "                            if vv is None or (isinstance(vv, str) and vv.strip() == \"\"):\n",
        "                                continue\n",
        "                            key = (section, k, str(vv))\n",
        "                            node = combined[key]\n",
        "                            node[\"section\"] = section; node[\"field\"] = k; node[\"value\"] = vv\n",
        "                            if page is not None: node[\"pages\"].add(str(page))\n",
        "                    else:\n",
        "                        key = (section, k, str(v))\n",
        "                        node = combined[key]\n",
        "                        node[\"section\"] = section; node[\"field\"] = k; node[\"value\"] = v\n",
        "                        if page is not None: node[\"pages\"].add(str(page))\n",
        "\n",
        "            elif isinstance(fields, list):\n",
        "                for item in fields:\n",
        "                    if item is None or (isinstance(item, str) and item.strip() == \"\"):\n",
        "                        continue\n",
        "                    key = (section, \"list_item\", str(item))\n",
        "                    node = combined[key]\n",
        "                    node[\"section\"] = section; node[\"field\"] = \"list_item\"; node[\"value\"] = item\n",
        "                    if page is not None: node[\"pages\"].add(str(page))\n",
        "\n",
        "    flat = [{\n",
        "        \"pdf_name\": info[\"pdf_name\"],\n",
        "        \"section\": info[\"section\"],\n",
        "        \"field\": info[\"field\"],\n",
        "        \"value\": info[\"value\"],\n",
        "        \"pages\": \", \".join(sorted(info[\"pages\"])) if info[\"pages\"] else \"\"\n",
        "    } for info in combined.values()]\n",
        "\n",
        "    return pd.DataFrame(flat, columns=[\"pdf_name\",\"section\",\"field\",\"value\",\"pages\"])\n",
        "\n",
        "# ---- Run over all per-PDF JSONs from Stage 3 ----\n",
        "json_paths = sorted(IN_DIR.glob(\"*.extracted.json\"))\n",
        "assert json_paths, f\"No Stage-3 JSONs found in {IN_DIR}. Run Stage 3 first.\"\n",
        "\n",
        "dfs = []\n",
        "for jp in json_paths:\n",
        "    # derive a friendly pdf name (e.g., \"Thesis.extracted.json\" -> \"Thesis.pdf\")\n",
        "    pdf_name = jp.stem.replace(\".extracted\", \"\") + \".pdf\"\n",
        "    raw = json.loads(jp.read_text(encoding=\"utf-8\"))\n",
        "    recs = normalize_records(raw)\n",
        "    df   = flatten_one_pdf(pdf_name, recs)\n",
        "\n",
        "    out_csv = OUT_DIR / f\"{jp.stem}.flatten.csv\"\n",
        "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"✓ {pdf_name}: {len(df)} rows → {out_csv}\")\n",
        "    dfs.append(df)\n",
        "\n",
        "# combined CSV\n",
        "combined_df = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame(columns=[\"pdf_name\",\"section\",\"field\",\"value\",\"pages\"])\n",
        "combined_df.to_csv(OUT_DIR / \"_combined_flatten.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"\\n✅ Combined flatten saved → {OUT_DIR / '_combined_flatten.csv'}\")\n",
        "\n",
        "# quick peek\n",
        "combined_df.head(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "US0jDjHu08G8",
        "outputId": "6d7ebc18-081e-4ae1-d332-a3e652a80c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ 2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf: 408 rows → /content/flattened_csv/2007_Tshibubudze_THE MARKOYE FAULT_2007.extracted.flatten.csv\n",
            "✓ 2008_MATABANE_FE3.pdf: 272 rows → /content/flattened_csv/2008_MATABANE_FE3.extracted.flatten.csv\n",
            "✓ 2011_Fougerouse_Masters_thesis.pdf: 364 rows → /content/flattened_csv/2011_Fougerouse_Masters_thesis.extracted.flatten.csv\n",
            "\n",
            "✅ Combined flatten saved → /content/flattened_csv/_combined_flatten.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      pdf_name        section  \\\n",
              "0  2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf       metadata   \n",
              "1  2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf       metadata   \n",
              "2  2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf       metadata   \n",
              "3  2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf       metadata   \n",
              "4  2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf       metadata   \n",
              "5  2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf       metadata   \n",
              "6  2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf  geochronology   \n",
              "7  2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf  geochronology   \n",
              "8  2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf   geochemistry   \n",
              "9  2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf    metallogeny   \n",
              "\n",
              "                 field                                              value  \\\n",
              "0                title  RELATIVE TIMING OF STRUCTURAL EVENTS: THE MARK...   \n",
              "1               author                                 ASINNE TSHIBUBUDZE   \n",
              "2                 year                                               2007   \n",
              "3           supervisor                                 Prof. Kim A.A Hein   \n",
              "4          institution                    University of the Witwatersrand   \n",
              "5             location                         Johannesburg, South Africa   \n",
              "6               age_Ma                                                  0   \n",
              "7             error_Ma                                                  0   \n",
              "8                value                                                  0   \n",
              "9  mineralisation_type                                               gold   \n",
              "\n",
              "                                               pages  \n",
              "0                                                  1  \n",
              "1                                                  1  \n",
              "2                                               1, 2  \n",
              "3                                                  1  \n",
              "4                                               1, 2  \n",
              "5                                                  1  \n",
              "6  1, 11, 13, 14, 15, 16, 17, 18, 19, 2, 20, 21, ...  \n",
              "7  1, 10, 11, 13, 14, 15, 16, 17, 18, 19, 2, 20, ...  \n",
              "8  1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2, ...  \n",
              "9                                        1, 13, 7, 8  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b5f2d1dd-cbdc-4795-b3e7-bfbaff2b4150\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>section</th>\n",
              "      <th>field</th>\n",
              "      <th>value</th>\n",
              "      <th>pages</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf</td>\n",
              "      <td>metadata</td>\n",
              "      <td>title</td>\n",
              "      <td>RELATIVE TIMING OF STRUCTURAL EVENTS: THE MARK...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf</td>\n",
              "      <td>metadata</td>\n",
              "      <td>author</td>\n",
              "      <td>ASINNE TSHIBUBUDZE</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf</td>\n",
              "      <td>metadata</td>\n",
              "      <td>year</td>\n",
              "      <td>2007</td>\n",
              "      <td>1, 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf</td>\n",
              "      <td>metadata</td>\n",
              "      <td>supervisor</td>\n",
              "      <td>Prof. Kim A.A Hein</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf</td>\n",
              "      <td>metadata</td>\n",
              "      <td>institution</td>\n",
              "      <td>University of the Witwatersrand</td>\n",
              "      <td>1, 2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf</td>\n",
              "      <td>metadata</td>\n",
              "      <td>location</td>\n",
              "      <td>Johannesburg, South Africa</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf</td>\n",
              "      <td>geochronology</td>\n",
              "      <td>age_Ma</td>\n",
              "      <td>0</td>\n",
              "      <td>1, 11, 13, 14, 15, 16, 17, 18, 19, 2, 20, 21, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf</td>\n",
              "      <td>geochronology</td>\n",
              "      <td>error_Ma</td>\n",
              "      <td>0</td>\n",
              "      <td>1, 10, 11, 13, 14, 15, 16, 17, 18, 19, 2, 20, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf</td>\n",
              "      <td>geochemistry</td>\n",
              "      <td>value</td>\n",
              "      <td>0</td>\n",
              "      <td>1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf</td>\n",
              "      <td>metallogeny</td>\n",
              "      <td>mineralisation_type</td>\n",
              "      <td>gold</td>\n",
              "      <td>1, 13, 7, 8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5f2d1dd-cbdc-4795-b3e7-bfbaff2b4150')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b5f2d1dd-cbdc-4795-b3e7-bfbaff2b4150 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b5f2d1dd-cbdc-4795-b3e7-bfbaff2b4150');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bd41d238-d821-44fc-9329-80057f6d473e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bd41d238-d821-44fc-9329-80057f6d473e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bd41d238-d821-44fc-9329-80057f6d473e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "combined_df",
              "summary": "{\n  \"name\": \"combined_df\",\n  \"rows\": 1044,\n  \"fields\": [\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2007_Tshibubudze_THE MARKOYE FAULT_2007.pdf\",\n          \"2008_MATABANE_FE3.pdf\",\n          \"2011_Fougerouse_Masters_thesis.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"section\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"geochronology\",\n          \"geology\",\n          \"geochemistry\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"field\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 26,\n        \"samples\": [\n          \"value\",\n          \"associated_structures\",\n          \"title\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"value\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 825,\n        \"samples\": [\n          \"cro\\u00fbte oc\\u00e9anique\",\n          \"olivine\",\n          \"pillow lavas\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pages\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 297,\n        \"samples\": [\n          \"1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 3, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 4, 40, 41, 42, 43, 44, 5, 6, 7, 8, 9\",\n          \"22, 23, 24, 29, 40\",\n          \"15, 19, 35, 43, 51\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}